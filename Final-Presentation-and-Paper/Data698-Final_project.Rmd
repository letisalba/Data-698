---
title: "Data 698 - Final Project"
author: "Leticia Salazar"
date: "January 8, 2024"
output:
  html_document:
    theme: united
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$~$

## [Polycystic Ovarian Syndrome (PCOS)]()

$~$

## Overview: 

Polycystic Ovarian Syndrome (PCOS) stands as one of the most prevalent endocrine disorders affecting reproductive-aged women globally. This complex condition disrupts hormone levels within the body, impacting the ovaries and often leading to a myriad of symptoms. PCOS is characterized by a spectrum of irregularities, including menstrual irregularities, hormonal imbalances, and the formation of cysts on the ovaries. Its manifestation varies among individuals, typically marked by excessive production of androgens, irregular menstrual cycles, and difficulties in ovulation, potentially leading to fertility challenges. Beyond reproductive health concerns, PCOS also presents with metabolic implications, such as insulin resistance, which can elevate the risk of developing type 2 diabetes and cardiovascular issues. The exact cause of PCOS remains multifaceted and isn't singularly defined, encompassing a combination of genetic, hormonal, and lifestyle factors. As a chronic disorder, PCOS poses significant challenges to those affected, impacting their overall health and quality of life.

This syndrome's diagnosis is often challenging due to its diverse array of symptoms and the absence of a singular definitive test. Clinicians typically rely on a set of diagnostic criteria, such as irregular menstrual cycles, elevated androgen levels, and the presence of ovarian cysts, to identify and evaluate PCOS. However, the complexity of this condition extends beyond its diagnostic challenges, as managing PCOS necessitates a multifaceted approach. Treatment strategies often focus on alleviating specific symptoms, such as irregular periods, infertility, and excessive hair growth, through medications, lifestyle modifications, and sometimes surgical interventions. Additionally, lifestyle changes involving diet adjustments and regular exercise play a crucial role in managing the metabolic aspects of PCOS. Research into PCOS continues to expand, aiming to enhance diagnostic accuracy, refine treatment approaches, and deepen our understanding of its underlying mechanisms to better support individuals grappling with this pervasive syndrome.

Hormones that are involved in PCOS are:

* **Androgens: ** aka "male hormones" are present in women with PCOS at higher levels than usual. Excess in androgens can cause symptoms such as acne, unwanted hair, thinning hair, and irregular periods.

* **Insulin: ** allows the body to absorb glucose (blood sugar) into the cells for energy. In PCOS, the body doesn't respond to insulin as intended therefore, elevations in blood glucose levels can be assessed. Such elevations then lead to increased production of androgen.

* **Progesterone: ** vital hormone for menstruation and pregnancy; lack of progesterone contributes to irregular periods.

$~$

PCOS Symptoms:

Many of these symptoms can be attributed to other causes or go unnoticed but it is very common for PCOS to go undiagnosed for some time. Here are some symptoms that help with the diagnosis:

* **Irregular periods: ** irregular or missed periods as are a result of not ovulating is a common signs of PCOS

* **Polycystic ovaries: ** some may develop cysts in their ovaries but some don't. Ovaries may be enlarged and follicles surrounding their eggs therefore failing to function regularly.

* **Excess androgen: ** elevated levels of male hormones can cause excess hair and acne.

$~$

Other symptoms may include:

* **Weight gain: ** many people with PCOS will have weight gain or obesity that is difficult to manage.

* **Fatigue: ** increase in fatigue or low energy is also common

* **Unwanted hair growth: ** due to excess androgen, areas such as face, arms, back, chest, hand, toes and abdomen may have excess hair growth.

* **Thinning hair on the head: ** hair loss may increase in middle age for those with PCOS

* **Infertility: ** PCOS is a leading cause for infertility but not everyone is the same.

* **Acne: ** due to hormonal changes, acne can be arise and make skin oilier than usual and cause breakout in the face, chest and upper back.

* **Darkening of skin: ** areas such as under arms, breasts or back of your neck may get dark, patchy or thicken

* **Mood changes: ** mood swings, depression and anxiety can increase

* **Pelvic pain: ** pain may occur with periods along with heavy bleeding or without bleeding

* **Headaches: ** can occur due to hormonal changes

* **Sleep problems: ** most people often suffer with problem such as insomnia or poor sleep. These arise due to many factors but a common one is having sleep apnea (sleep disorder). Even when you fall asleep you wake up as if you have not slept at all or have trouble falling asleep.

* **Depression: ** can arise due to symptoms that can alter your appearance and have a negative impact on your emotions.

**It's good to note that not everyone who is diagnosed with PCOS experiences all of these symptoms and should always consult with a their PCP or OBGYN to get an accurate diagnosis.**

$~$

## The Problem:

Early identification of risk factors associated with PCOS can assist in timely interventions and lifestyle adjustments. Tailoring suggestions or treatments based on individualized risk profiles has the potential to enhance patient outcomes. Employing predictive models can play a pivotal role in increasing awareness regarding PCOS risk factors and preventive measures. However, limitations may exist in accessing comprehensive and varied datasets containing accurate demographic, clinical, and lifestyle data. It is crucial to ensure the model's transparency and interpretability to facilitate well-informed decisions and recommendations. Additionally, it's imperative that the model demonstrates proficiency across diverse demographic groups and populations. Addressing these challenges involves the development of a robust predictive model using machine learning techniques. Such an endeavor holds the promise of significantly contributing to the identification of individuals at risk of PCOS, thereby enabling early interventions and guiding personalized healthcare strategies for improved management of the condition. This project will investigate publicly available datasets that will be used to develop machine learning models that predicts the probability or risk of an individual having or developing PCOS based on demographic information (age, ethnicity, geographical location), clinical data (hormonal levels, BMI, menstrual irregularities), and lifestyle factors (dietary habits, exercise routine, stress levels).


$~$

## The Data:

The Polycystic ovary syndrome (PCOS) dataset, available on [Kaggle.com](https://www.kaggle.com/datasets/prasoonkottarathil/polycystic-ovary-syndrome-pcos), is comprised of two csv files labeled `PCOS_data_without_infertility` and `PCOS_infertility`. In total, these files encompass 48 variables and 541 data entries all collected from 10 different hospitals across Kerala, India. The dataset contains all physical and clinical parameters to determine PCOS and infertility related issues.

**Full description of the variables below:**

* Units used range from imperial to metric system of measurement

* For Yes | No questions
  * Yes = 1
  * No = 0

Variables                               Description
----------------------------            ---------------------------------------
"Sl..No"                                unique identification number assigned to each entry
"Patient.File.No."                      file number for each patient's record.       
"PCOS..Y.N."                            presence or absence of PCOS
"I...beta.HCG.mIU.mL."                  pregnancy hormone case I measured in 
                                        milli-international units per liter (mIU/L)
"II....beta.HCG.mIU.mL."                pregnancy hormone case II measured in 
                                        milli-international units per liter (mIU/L)
"AMH.ng.mL."                            detects ovarian reserve (egg count)           
"Age..yrs."                             age of patient in years            
"Weight..Kg."                           weight of patient in kg            
"Height.Cm."                            height of patient in cm            
"BMI"                                   body mass index                    
"Blood.Group"                           Blood Groups: 
                                        A+ = 11, A- = 12, B+ = 13, B- = 14, 
                                        O+ = 15, O- = 16, AB+ = 17, AB- = 18            
"Pulse.rate.bpm."                       beats per minute       
"RR..breaths.min."                      respiration rates per minute       
"Hb.g.dl."                              hemoglobin concentration measured in 
                                        grams per deciliter (g/dL).               
"Cycle.R.I."                            cycle Regularity Index used to assess the 
                                        regularity or irregularity of menstrual cycles 
                                        in women: 4 indicates irregular menstrual cycle, 
                                        2 indicates a regular menstrual cycle           
"Cycle.length.days."                    length of menstrual cycle     
"Marraige.Status..Yrs."                 years married  
"Pregnant.Y.N."                         pregnant yes or no         
"No..of.aborptions"                     number of abortions      
"FSH.mIU.mL."                           follicle stimulating hormone measured 
                                        in milli-international units per liter (mIU/L)            
"LH.mIU.mL."                            luteinizing hormone (increases during ovulation) 
                                        measured in milli-international units per liter (mIU/L)             
"FSH.LH"                                ratio between Follicle-Stimulating Hormone (FSH) 
                                        and Luteinizing Hormone (LH)                
"Hip.inch."                             measurement of hips in inches              
"Waist.inch."                           measurement of waist in inches            
"Waist.Hip.Ratio"                       ratio of measurement of waist and hip       
"TSH..mIU.L."                           thyroid stimulating hormone measured in 
                                        milli-international units per liter (mIU/L)           
"AMH.ng.mL."                            Anti-Müllerian Hormone (AMH) measured in 
                                        nanograms per milliliter (ng/mL); a marker used in 
                                        reproductive medicine to assess ovarian reserve             
"PRL.ng.mL."                            Prolactin measured in nanograms per milliliter (ng/mL); 
                                        a hormone produced by the pituitary gland           
"Vit.D3..ng.mL."                        Vitamin D3 measured in nanograms per milliliter (ng/mL); 
                                        is essential for bone health, immune function, and 
                                        various other bodily processes.         
"PRG.ng.mL."                            Progesterone measured in nanograms per milliliter (ng/mL); a
                                        hormone involved in the menstrual cycle, pregnancy, and
                                        maintaining the uterine lining for a developing embryo.             
"RBS.mg.dl."                            Random Blood Sugar measured in milligrams per deciliter 
                                        (mg/dL); it represents the level of glucose (sugar) present
                                        in the blood at a random time, without fasting.            
"Weight.gain.Y.N."                      weight gain yes or no       
"hair.growth.Y.N."                      hair growth yes or no (hirsutism)       
"Skin.darkening..Y.N."                  darkening of skin yes or no  
"Hair.loss.Y.N."                        hair loss yes or no         
"Pimples.Y.N."                          pimples (acne) yes or no           
"Fast.food..Y.N."                       consumption of fast food yes or no    
"Reg.Exercise.Y.N."                     regularly exercise yes or no                 
"BP._Systolic..mmHg."                   systolic blood pressure measured in millimeters of mercury (mmHg)     
"BP._Diastolic..mmHg."                  diastolic blood pressure measured in millimeters of mercury (mmHg)  
"Follicle.No...L."                      number of follicles on left ovary       
"Follicle.No...R."                      number of follicles in right ovary       
"Avg..F.size..L...mm."                  average size of follicles in left ovary measured in millimeters (mm)  
"Avg..F.size..R...mm."                  average size of follicles in right ovary measured in millimeters (mm)  
"Endometrium..mm."                      size of the endometrial thickness in millimeters (mm)

$~$

### Load Libraries:

The following libraries were utilized in this assignment:

```{r, warning=FALSE, message=FALSE, cache=FALSE, comment=FALSE}
# load libraries
library(tidyverse) # data prep
library(DataExplorer) # histograms for datasets
library(skimr) # data prep
library(rpart) # decision tree package
library(rpart.plot) # decision tree display package
library(kableExtra) # kable function for tables 
library(knitr) # kable function for table
library(tidyr) # splitting data
library(ggplot2) # graphing
library(hrbrthemes) # chart customization
library(gridExtra) # layering charts
library(stringr) # data prep
library(tidymodels) # predictions
library(corrplot) # correlation plot
library(randomForest) # for the random forest
library(caret) # confusion matrix
library("e1071") #svm
library(formattable) 
library(corrplot) # correlation plot
library(caret) # confusion matrix
library(neuralnet) # neural network
library(stats) # linear and logistic regression
library(gbm) # generalized boosted models
library(xgboost) # extreme gradient boosting
library(kknn) # weighted k-Nearest neighbors
library(jtools)  # use of summ()
library(patchwork) # ggplot2 multiplot title
library(class) # knn function
```

$~$

### Load data:

The dataset selected has been incorporated into my [GitHub](https://github.com/letisalba/Data-698/tree/master/Data-Collection-and-Analysis) and imported into R.

```{r, echo=FALSE}
# load the dataset from github
pcos <- read.csv("https://raw.githubusercontent.com/letisalba/Data-698/master/Data-Collection-and-Analysis/csv/PCOS_infertility.csv")
pcos2 <- read.csv("https://raw.githubusercontent.com/letisalba/Data-698/master/Data-Collection-and-Analysis/csv/PCOS_without_infertility.csv")
```

$~$

#### Displaying the `pcos` data:
```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}
# display the `pcos` dataset
pcos %>% 
  kable(caption = "<font color=#000000><b>Table 1.</b>`pcos` data </font>", format = "html", col.names = colnames(pcos)) %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```
$~$

$~$

#### Displaying the `pcos2` data:
```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}
# display the `pcos2` dataset
pcos2 %>% 
kable(caption = "<font color=#000000><b>Table 2.</b>`pcos2` data </font>", format = "html", col.names = colnames(pcos2)) %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

### Data Exploration:

Using the `skimr` library we can obtain a quick summary statistic of the dataset. The first data set `pcos` includes 541 observations and a total of 6 variables. The second data set `pcos2` includes 541 observations and a total of 45 variables. There seems to be no missing values in both datasets but there is a mixture of character and numeric column types that have to be addressed / calculated. Notice that the column names are not clear enough for readers, this will be tackled in the data preparation section.

$~$

#### Summary of the `pcos` data:
```{r, echo=FALSE}
# summary of the pcos dataset
skim(pcos)
```

$~$

#### Summary of the `pcos2` data:
```{r, echo=FALSE}
# summary of the pcos2 dataset
skim(pcos2)
```

$~$


$~$


Histograms provide valuable visual insights into the distribution patterns within a dataset. With the use of the `Data Explorer` library it allows us to effortlessly generate histograms that showcase the distribution characteristics of the entire dataset, as illustrated below.

$~$

#### `pcos` data:

Upon analyzing the variable distribution, my initial observation indicates that the variables follow a right-skewed distribution. Although the `SI..No` variable exhibits a unimodal distribution, it lacks significance and will be eliminated upon merging both datasets.

```{r, r, fig.height=4, fig.width=6, fig.align='center', warning=FALSE, echo=FALSE, message=FALSE}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 1, fill = "#7e102c"),
  title = "Fig. 1 - Histogram of `pcos` data",
   data = pcos,
         ggtheme=theme_ipsum())
```

$~$

#### `pcos2` data:

Upon visual inspection, no distinct pattern or discernible shape emerges from the histograms for the second dataset as a whole. Similar to the first dataset, `SI..No` and `Patient.File.No.` has a unimodel distribution but plays no significant role in my analysis.

```{r, fig.height = 10, fig.width = 10, fig.align='center', warning=FALSE, echo=FALSE, message=FALSE}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 1, fill = "#a86800"),
  title = "Fig. 2 - Histogram of `pcos2` data",
   data = pcos2,
         ggtheme=theme_ipsum())
```

$~$

I would like to further examine these variables but not before preparing the data set to ensure all variables are accounted for especially for `BMI`, `FSH.LH` and `Waist.Hip.Ratio`.

$~$

### Data Preparation:

My data preparation primarily involves column renaming and addressing missing values. This includes handling missing values by employing mean, median, or mode replacement strategies for columns like `	Marraige.Status..Yrs.` and `Fast.food..Y.N.`. Additionally, for columns such as `BMI`, `FSH.LH`, and `Waist.Hip.Ratio`, I will compute overall values. Specific columns will be converted to the metric system, and redundant or unnecessary columns will be removed. These steps aim to streamline data management before merging the two datasets.

$~$

Prior to commencing the cleaning process, I standardized the datasets to numeric format due to variations in the class of certain variables.

```{r, echo=FALSE, warning=FALSE}
# change variables to numeric
pcos <- mutate_all(pcos, function(x) as.numeric(as.character(x)))
pcos2 <- mutate_all(pcos2, function(x) as.numeric(as.character(x)))
```

$~$

From the `skimr` summary it was noted that there were no missing values but there was an error in the computational values of certain columns. Below is the initial count of missing values for each data set:

`pcos` dataset:

```{r, echo=FALSE}
# missing data
colSums(is.na(pcos))
```
$~$

`pcos2` dataset:
```{r, echo=FALSE}
# missing data
colSums(is.na(pcos2))
```

$~$


```{r, echo=FALSE}
# removing first two column for `pcos` data
pcos <- dplyr::select(pcos, -c(2:6))

# renaming columns for `pcos` data
pcos <- pcos %>% 
  rename("Sl.No" = "Sl..No")

# removing columns not needed for `pcos_infertility` data
pcos2 <- dplyr::select(pcos2, -c(2, 45))

# renaming columns for `pcos_infertility` data
pcos2 <- pcos2 %>% 
  rename("Sl.No" = "Sl..No",
         "PCOS" = "PCOS..Y.N.",
         "Age_yrs" = "Age..yrs.",
         "Weight" = "Weight..Kg.",
         "Height" = "Height.Cm.",
         "BMI" = "BMI",
         "Blood_Group" = "Blood.Group", 
         "Pulse_rate_bpm" = "Pulse.rate.bpm.",
         "RR_breaths_min" = "RR..breaths.min.",
         "Hb_gdl" = "Hb.g.dl.",
         "Cycle_RI" = "Cycle.R.I.",
         "Cycle_length_days" = "Cycle.length.days.",
         "Married_yrs" = "Marraige.Status..Yrs.",
         "Pregnant" = "Pregnant.Y.N.",
         "No_of_abortions" = "No..of.aborptions",
         "IbetaHCG_mIUmL" = "I...beta.HCG.mIU.mL.",
         "IIbetaHCG_mIUmL" = "II....beta.HCG.mIU.mL.",
         "FSH_mIUmL" = "FSH.mIU.mL.",
         "LH_mIUmL" = "LH.mIU.mL.",
         "FSH_LH" = "FSH.LH",
         "Hip" = "Hip.inch.",
         "Waist" = "Waist.inch.",
         "Waist_Hip_Ratio" = "Waist.Hip.Ratio",
         "TSH_mIUmL" = "TSH..mIU.L.",
         "AMH_ngmL" = "AMH.ng.mL.",
         "PRL_ngmL" = "PRL.ng.mL.",
         "Vit_D3_ngmL" = "Vit.D3..ng.mL.",
         "PRG_ngmL" = "PRG.ng.mL.",
         "RBS_mgdl" = "RBS.mg.dl.",
         "Weight_gain" = "Weight.gain.Y.N.",
         "Hair_growth" = "hair.growth.Y.N.",
         "Skin_darkening" = "Skin.darkening..Y.N.",
         "Hair_loss" = "Hair.loss.Y.N.",
         "Pimples" = "Pimples.Y.N.",
         "Fast_food" = "Fast.food..Y.N.",
         "Reg_Exercise" = "Reg.Exercise.Y.N.",
         "BP_Systolic_mmHg" = "BP._Systolic..mmHg.",
         "BP_Diastolic_mmHg" = "BP._Diastolic..mmHg.",
         "Follicle_NoL" = "Follicle.No...L.",
         "Follicle_NoR" = "Follicle.No...R.",
         "Avg_F_sizeL_mm" = "Avg..F.size..L...mm.",
         "Avg_F_sizeR_mm" = "Avg..F.size..R...mm.",
         "Endometrium_mm" = "Endometrium..mm.")
```

$~$

After making the first part of addressing the column names, the results are seen below of the merged datasets: 

```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}
# merge data sets
pcos_data <- merge(pcos, pcos2,  by=c("Sl.No"))
# display the merged dataset
pcos_data %>% 
kable(caption = "<font color=#000000><b>Table 3.</b>`pcos_data` merged data </font>", format = "html", col.names = colnames(pcos_data)) %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

In the second part, I addressed missing values by initially converting `Height` from centimeters to meters, `Hip` and `Waist` from inches to cm and computing `BMI`, `Waist_Hip_Ratio`, and `FSH_LH`. Following a thorough assessment of the dataset, I opted to substitute missing values in `Married_yrs`, `AMH_ngmL`, and `Fast_food` with the median value as it didn't significantly affect the data's distribution.


```{r, echo=FALSE, warning=FALSE}
# convert Height from cm to meters
pcos_data$"Height" <- round((pcos_data$"Height" * 0.01),1)

# convert hip and waist from inches to cm
pcos_data$"Hip" <- round((pcos_data$"Hip" * 2.54),1)
pcos_data$"Waist" <- round((pcos_data$"Waist" * 2.54),1)

#calculate BMI
pcos_data$"BMI" <- round((pcos_data$"Weight" / pcos_data$"Height"^2), 1)

# calculate waist-hip ratio
pcos_data$"Waist_Hip_Ratio" <- round((pcos_data$"Waist" / pcos_data$"Hip"),2)

# calculate FSH/LH
pcos_data$"FSH_LH" <- round((pcos_data$"FSH_mIUmL"/pcos_data$"LH_mIUmL"),2)

# calculate Married years
pcos_data$"Married(yrs)"[is.na(pcos_data$"Married(yrs)")] <- median(pcos_data$"Married(yrs)", na.rm = T)

# calculate Fast food
pcos_data$"Fast_food"[is.na(pcos_data$"Fast_food")] <- median(pcos_data$"Fast_food", na.rm = T)

# calculate AMH_ngml
pcos_data$"AMH_ngmL"[is.na(pcos_data$"AMH_ngmL")] <- median(pcos_data$"AMH_ngmL", na.rm = T)

# List of variables to round
vars_to_round <- c("Hb_gdl", "Married_yrs", "IbetaHCG_mIUmL", "IbetaHCG_mIUmL", 
                   "FSH_mIUmL", "LH_mIUmL", "FSH_LH", "TSH_mIUmL", "AMH_ngmL", "PRL_ngmL", 
                   "Vit_D3_ngmL", "PRG_ngmL", "Avg_F_sizeR_mm", "Endometrium_mm")

# Rounding the variables to 1 decimal places
pcos_data <- pcos_data %>%
  mutate_at(vars(vars_to_round), ~ round(., digits = 1))


# remove 1st column
pcos_cleaned <- pcos_data[-1]
```

$~$

#### The final results are seen in `pcos_cleaned` below:
```{r,echo=FALSE}
# display results of cleaned pcos
pcos_cleaned %>% 
kable(caption = "<font color=#000000><b>Table 4.</b>`pcos_cleaned` dataset </font>", format = "html", col.names = colnames(pcos_cleaned)) %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

Once more, I generated a `DataExplorer` histogram to inspect the impact of the alterations on the distribution. While more variables have been included in the distribution, there isn't a consistent pattern; each variable demonstrates variations, displaying either a normal distribution, right-skewed, left-skewed, or no discernible pattern.

```{r, fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center', warning=FALSE, eval=TRUE, message=FALSE}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 1, fill = "dark blue"),
  title = "Fig. 3 - Histogram of `pcos_cleaned` data",
   data = pcos_cleaned,
         ggtheme=theme_ipsum())
```

$~$

The correlation plot below is measuring the degree of linear relationship within the dataset. The values in which this is measured falls between -1 and +1, with +1 being a strong positive correlation and -1 a strong negative correlation. The darker the dot the more strongly correlated (whether positive or negative). Based on the findings below, there aren't many variables that exhibit a notably strong positive or negative correlation, although some variables do fall within these categories to some extent.

Some notable correlations are as follows:

* Positive correlations between: `BMI` and `Weight`, `Weight_gain`, `Hair_growth`, `Skin_darkening`and `PCOS` and `Follicle_NoL`, `Follicle_NoR`, and `PCOS` to name a few.

* Negative correlations between: `Waist_Hip_Ratio` and `Hip`, `AMH_ngmL` and `Age_yrs`, and `Follicle_NoR`, `Follicle_NoL` and`Age_yrs`.

```{r, fig.height=10, fig.width=10, warning=FALSE, echo=FALSE, message=FALSE}
# Selecting only the numerical variables for correlation
numerical_data <- pcos_cleaned[, sapply(pcos_cleaned, is.numeric)]

# Calculating the correlation matrix
cor_matrix <- cor(numerical_data)

# Print the correlation matrix
corrplot(cor_matrix, method = "color", type = "lower", 
         tl.col = "black", tl.cex = 0.9, title = "Fig. 4 Correlation plot of `pcos_cleaned` data",mar=c(0,0,1,0))#http://stackoverflow.com/a/14754408/54964
```

$~$

Outliers, while sometimes seen as anomalies or errors, hold significant importance in data analysis for various reasons. They can substantially influence statistical measures, potentially skewing interpretations of central tendency (mean, median, mode) and dispersion (variance, standard deviation). Their presence might distort the true characteristics of the data distribution. Outliers can adversely affect the performance of predictive models. Many machine learning algorithms are sensitive to outliers, leading to biased predictions or reduced model accuracy. Identifying and handling outliers appropriately is crucial for robust model building. Recognizing outliers aids in improving data quality by either correcting errors or understanding exceptional cases in the dataset. If handled properly, outliers can enhance our understanding of the dataset, improve model performance, and contribute to more accurate and meaningful insights. Detecting and managing outliers is essential for maintaining the integrity and reliability of data-driven analyses. With the help of boxplots we can identify outliers  across a complete dataset. 

According to the visualizations provided, a handful of outliers are evident. Considering the dataset's relatively small size, I've chosen to retain these outliers to ensure the inclusion of every individual data point as it represents natural variations in the population.

```{r, fig.height=18, fig.width=10, warning=FALSE, echo=FALSE, message=FALSE, fig.align='center'}

# boxplot of the variables with the outlier parameters
pcos_df2 <- pcos_cleaned %>%
                gather(variable, values, 1:dim(pcos_cleaned)[2])
pcos_df2 %>%
  ggplot() +
  geom_boxplot(aes(x = variable, y = values)) +
  facet_wrap(~variable, ncol = 4, scales = "free") +
  ggtitle("Fig. 5 - Boxplot outliers for `pcos_cleaned` data") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=14)
    )
```

$~$

#### Further visualizations:

Before diving into the model-building phase of this project, let's further visualize the variables with a series of scatterplots, histograms and bar charts.

* There is a wide range of distribution between the age of these women most falling between 23 to 38 years old. Similarly, weight also has a wide range of distribution.

* Most of the women in this study are between 1.5 - 1.6 meters tall (4’9” - 5’2”).

* The distribution of years married also varies where the majority of the women fall between the first 10 years.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=8, fig.width=15, fig.align='center'}

# Histogram of Age distribution 
p1 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(`Age_yrs`))) +
  geom_histogram(stat = "count", fill = "#F39C12", color = "#e9ecef", alpha = 0.9) +
  geom_text(stat = "count", aes(label = ..count..), position = position_dodge(width = 1), vjust = -0.5, color = "black") +
  labs(title = "Age Distribution", x = "Age (years)", y = "Count") +
  theme_ipsum() +
  theme(
    plot.title = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 1, hjust=2, size=5)
  )

# Histogram of Weight distribution
p2 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Weight`))) +
  geom_histogram(stat = "count", fill = "#FF5733", color = "#e9ecef", alpha = 0.9) +
  geom_text(stat = "count", aes(label = ..count..), position = position_dodge(width = 1), vjust = -0.5, color = "black") +
    labs(title="Weight Distribution", x ="Weight (kg)", y = "Count") +
    theme_ipsum() +
      theme(
        plot.title = element_text(size=12), axis.text.x = element_text(angle = 90, vjust = 1, hjust=2, size=5)
    )

# Histogram of years married distribution
p3 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Married_yrs`))) +
    geom_histogram(stat="count", show.legend = FALSE, fill = "#C70039", color = "#e9ecef", alpha=0.9) +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    labs(title="Years Married Distribution", x ="Married (yrs)", y = "count") +
    theme_ipsum() +
      theme(
        plot.title = element_text(size=12), axis.text.x = element_text(angle = 90, vjust = 1, hjust=2, size=5)
    )

# plot all histograms
ggp_all <- (p1 + p2 + p3) +
  plot_annotation(
    title = 'Fig. 6 - Scatterplot of Biometric measures Variables',
    theme = theme(plot.title = element_text(hjust = 0.5, size = 16))
  )
ggp_all 
```

$~$

Various physiological and hormonal patterns within the dataset and related to PCOS show that: 

* The age and weight of the women are distributed evenly throughout as opposed to being clustered in a certain age and weight group.

* Hip to waist counts increase for those with and without PCOS.

* Cycle length seems to be very consistent regardless of PCOS diagnosis.

* Women with PCOS experience more irregular periods with a few with no PCOS do as well.

* With or without PCOS women’s BMI fluctuates in women of all ages.

* The distribution of number of follicles is greater for those women without PCOS.

* The distribution of size in follicles left or right don't differ as greatly as I thought for women with and without PCOS.

* The variance in endometrial thickness between women with PCOS (thinner) and those without PCOS (thicker) evolves as the menstrual cycle progresses. This discrepancy arises due to hormonal imbalances. In women affected by PCOS, the absence of regular menstrual cycles leads to an unaltered endometrial lining, contrasting with the changes observed in women without the condition.

* Pregnancy hormones are clustered with the exceptions of some outliers which were initially noted.

* Blood pressure levels, Pulse rate and Respiration rate for women with and without PCOS are in normal range.

* The quantity of eggs and the development of follicles show similar patterns between women self-reporting PCOS and those without, yet there is no discernible correlation between them.


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=12, fig.width=12, fig.align='center'}

# Scatterplot of Age and Weight
p4 <- pcos_cleaned %>% 
  ggplot(aes(x=`Age_yrs`, y=`Weight`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Age(yrs) with Weight(kg)",
        x ="Age(yrs)", y = "Weight(kg)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of Hip and Waist
p5 <- pcos_cleaned %>% 
  ggplot(aes(x=`Hip`, y=`Waist`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Relationship between Hip(cm) and Waist(cm)",
        x ="Hip (cm)", y = "Waist (cm)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of Length of Cycle and Age
p6 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`Cycle_length_days`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Length of cycle based on Age",
        x ="Age(yrs)", y = "Cycle Length(days)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of Type of Cycle and Age
p7 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`Cycle_RI`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Type of cycle based on Age",
        x ="Age(yrs)", y = "Cycle(R/I)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of BMI and Age
p8 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=BMI, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="BMI based on Age",
        x ="Age(yrs)", y = "BMI") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of number of follicles in left and right ovaries and PCOS 
p9 <- pcos_cleaned %>% 
ggplot(aes(x=`Follicle_NoR`, y=`Follicle_NoL`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Number of Follicles",
        x ="Follicles Right", y = "Follicles Left") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of follicle size and PCOS
p10 <- pcos_cleaned %>% 
ggplot(aes(x=`Avg_F_sizeR_mm`, y=`Avg_F_sizeL_mm`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Follicle size's",
        x ="Follicle size right", y = "Follicle size left") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of Endometrium and Length of Cycles
p11 <- pcos_cleaned %>% 
ggplot(aes(x=`Endometrium_mm`, y=`Cycle_length_days`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Endometrium and length of cycle(days)",
        x ="Endometrium(mm)", y = "Cycle Length(days)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of pregnancy hormone levels
p12 <- pcos_cleaned %>% 
ggplot(aes(x=`IbetaHCG_mIUmL`, y=`IIbetaHCG_mIUmL`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Pregnancy hormone levels",
        x ="I Beta-HCG(mIu/mL)", y = "II Beta-HCG(mIu/mL)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of blood pressure levels against PCOS
p13 <- pcos_cleaned %>% 
ggplot(aes(x=`BP_Diastolic_mmHg`, y=`BP_Systolic_mmHg`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Blood Pressure levels",
        x ="BP_Diastolic(mmHg)", y = "BP_Systolic(mmHg)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot of Respiration rate and Pulse rate against PCOS 
p14 <- pcos_cleaned %>% 
ggplot(aes(x=`RR_breaths_min`, y=`Pulse_rate_bpm`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Respiration rate vs Pulse rate(bpm)",
        x ="RR(breaths/min)", y = "Pulse rate(bpm)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# Scatterplot on Ovarian reserve against PCOS
p15 <- pcos_cleaned %>% 
ggplot(aes(x=`AMH_ngmL`, y=`FSH_LH`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Ovarian reserve against follicle growth",
        x ="Ovarian reserve", y = "Growth of ovarian follicles") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

# plot all scatterplots
ggp_all2 <- (p4 + p5 + p6) / (p7 + p8 + p9) / (p10 + p11 + p12) / (p13 + p14 + p15) + 
    plot_annotation(
    title = 'Fig. 7 - Scatterplot of variables with PCOS (Y/N) as factor',
    theme = theme(plot.title = element_text(hjust = 0.5, size = 16))
  )
ggp_all2  
```

$~$

Out of 541 women:

* 32.72% reported to have PCOS

* 38.08% of women reported to being pregnant

* 37.71% reported to experience weight gain

* 27.36% reported to experience hair growth

* 30.68% experience skin darkening

* 45.29% reported to experience hair loss

* 48.98% reported to experience pimples

* 51.57% reported to have fast food

* 24.77% reported to exercise regularly

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=22, fig.width=18, fig.align='center'}

# barchart of PCOS variable
p16 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`PCOS`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="PCOS", x ="PCOS(Yes or No)", y = "count") +
   theme_ipsum() +
      theme(
        plot.title = element_text(size=12)
      )

# barchart of Pregnant variable
p17 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Pregnant), fill = as.factor(Pregnant))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Pregnant", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Pregnant", x ="Pregnant(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Weight gain variable
p18 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Weight_gain), fill = as.factor(Weight_gain))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Weight Gain", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Weight Gain", x ="Weight Gain (Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Hair growth variable
p19 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Hair_growth), fill = as.factor(Hair_growth))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Hair Growth", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Hair Growth", x ="Hair Growth(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Skin darkening variable
p20 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Skin_darkening), fill = as.factor(Skin_darkening))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Skin Darkening", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Skin Darkening", x ="Skin Darkening (Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Hair loss variable
p21 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Hair_loss), fill = as.factor(Hair_loss))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Hair Loss", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Hair Loss", x ="Hair Loss(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Pimples variable
p22 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Pimples), fill = as.factor(Pimples))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Pimples", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Pimples", x ="Pimples(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Fast food variable
p23 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Fast_food), fill = as.factor(Fast_food))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Fast Food", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Fast Food", x ="Fast Food(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# barchart of Regularly Exercise variable
p24 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Reg_Exercise), fill = as.factor(Reg_Exercise))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Regular Exercise", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Regularly Exercise", x ="Regular Exercise(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )

# plot all barcharts
ggp_all3 <- (p16 + p17 + p18) / (p19 + p20 + p21) / (p22 + p23 + p24) +
    plot_annotation(
    title = 'Fig. 8 - Bar charts of Bloodwork variables',
    theme = theme(plot.title = element_text(hjust = 0.5, size = 18))
  )
ggp_all3 
```

$~$

Taking into account the presence of PCOS within the study:

* There's prevalence of O+, B+, and A+ blood types among the majority of women involved. Furthermore, women possessing blood types A+, B+, and O+ reported a higher incidence of PCOS compared to those with blood types A-, B-, O-, AB+, and AB-.

* Being that PCOS and infertility are linked, there are 11.83% of women who reported to being pregnant with PCOS.

* 22.37% of women reported to experience weight gain with PCOS.

* 18.67% of women reported to experience hair growth with PCOS.

* 20.33% of women reported to experience skin darkening with PCOS.

* 18.85% of women reported to experience hair loss with PCOS.

* 22.74% of women reported to experience pimples with PCOS.

* 25.13% of women reported to consume fast food with PCOS.

* 9.43% of women reported to regularly exercise with PCOS.

* Not surprised to see how fluctuated the cycle length is especially for women without PCOS since they tend to have a more regular cycle.

* Abortions were also included in this study but there's no evidence that it's influenced by PCOS; more women without PCOS experience 1 or more abortions.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=22, fig.width=18, fig.align='center'}

# barchart of Blood Group variable against PCOS 
p25 <- pcos_cleaned %>% 
  ggplot(aes(x = Blood_Group, fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Blood Group with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Pregnant variable against PCOS
p26 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Pregnant), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Pregnant with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Weight gain variable against PCOS
p27 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Weight_gain), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Weight gain with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Hair growth variable against PCOS
p28 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Hair_growth), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Hair growth with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Skin darkening variable against PCOS
p29 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Skin_darkening), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Skin darkening with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Hair loss variable against PCOS
p30 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Hair_loss), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Hair loss with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Pimples variable against PCOS
p31 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Pimples), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Pimples with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Fast food variable against PCOS
p32 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Fast_food), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Fast food consumption with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Reg. Exercise variable against PCOS
p33 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Reg_Exercise), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Regularly exercises with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Length of Cycle variable against PCOS
p34 <- pcos_cleaned %>% 
  ggplot(aes(x = `Cycle_length_days`, fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Cycle length in days with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# barchart of Number of abortions variable against PCOS
p35 <- pcos_cleaned %>% 
  ggplot(aes(x = No_of_abortions, fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    ggtitle("Number of abortions with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=12), axis.text.x = element_text(angle = 45)
    )

# plot all barcharts
ggp_all4 <- (p25 + p26 + p27) / (p28 + p29 + p30) / (p31 + p32 + p33) / (p34 + p35) + 
    plot_annotation(
    title = 'Fig. 9 - Bar charts of yes or no variables',
    theme = theme(plot.title = element_text(hjust = 0.5, size = 18))
  )
ggp_all4 
```

$~$

Within hormonal markers: 

* Vitamin D3 levels are spread out for those who reported with or without PCOS.

* FSH/LH levels are right skewed for those who reported with or without PCOS.

* TSH levels are also spread out for women who reported to have PCOS and those who don’t.

* Hemoglobin levels seem to be higher for those who reported to have PCOS.

* PRL levels (prolactin in the blood) are consistent at 1 or 2 for those with or without PCOS.

* RBS (random glucose) is fairly distributed with those with and without PCOS.

```{r, echo=FALSE, warning=FALSE, fig.height=10, fig.width=12, message=FALSE}

# Barchart for Vitamin D3 levels with PCOS
p36 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Vit_D3_ngmL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Vitamin D3 levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

# Barchart for FSH/LH levels with PCOS 
p37 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`FSH_LH`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("FSH/LH levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )


# Barchart for Thyroid Hormone levels with PCOS
p38 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`TSH_mIUmL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Thyroid Hormone levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

# Barchart of hemoglobin levels with PCOS
p39 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Hb_gdl`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Hemoglobin levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

# Barchart of Prolactin levels with PCOS
p40 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`PRL_ngmL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Prolactin levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

# Barchart of Progesterone levels with PCOS
p41 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`PRG_ngmL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Progesterone levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

# Barchart of Glucose levels with PCOS
p42 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`RBS_mgdl`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Glucose levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

# plot barcharts
ggp_all5 <- (p36 + p37 + p38 + p39) / (p40 + p41 + p42)  +
    plot_annotation(
    title = 'Fig. 10 - Bar charts of Biometrics Measures including PCOS (Y/N) as factor',
    theme = theme(plot.title = element_text(hjust = 0.5, size = 16))
  )
ggp_all5 
```

$~$


### Model Building:

As previously mentioned, I've decided for the prediction anaylsis to do Decision Tree (rpart), Random Forest (randomForest), Gradient Boosting Machines (xgboost or gbm), Support Vector Machines (e1071), Neural Networks (neuralnet), and K-Nearest Neighbors (kknn or class). 

Decision Tree (rpart): Decision trees recursively split the dataset based on features to make decisions. They represent a tree-like structure where nodes correspond to features, branches represent decisions, and leaves denote outcomes. R's rpart package builds decision trees using the Recursive Partitioning and Regression Trees algorithm.

Random Forest (randomForest): Random Forest is an ensemble learning method that constructs multiple decision trees and merges their predictions to improve accuracy and reduce overfitting. The randomForest package in R implements this ensemble method, creating a collection of decision trees and aggregating their predictions.

Gradient Boosting Machines (xgboost or gbm): Gradient Boosting Machines (GBMs) create an ensemble of weak learners (typically decision trees) sequentially, where each new tree corrects the errors made by the previous one. R provides packages like xgboost and gbm to implement gradient boosting, allowing users to build efficient and optimized boosting models.

Support Vector Machines (e1071): Support Vector Machines are supervised learning models used for classification or regression tasks. They find a hyperplane that best separates data into different classes while maximizing the margin. The e1071 package in R includes SVM implementations for classification and regression tasks.

Neural Networks (neuralnet): Neural Networks are a set of algorithms designed to recognize patterns. They consist of interconnected nodes (neurons) organized in layers and can model complex relationships in data. The neuralnet package in R allows users to create and train neural networks for various tasks like regression and classification.

K-Nearest Neighbors (kknn or class): K-Nearest Neighbors (KNN) is a simple, instance-based learning algorithm. It classifies new data points based on majority voting from the k-nearest data points in the training set. R provides packages like kknn or class to implement KNN algorithms for both classification and regression tasks.

$~$

We start by splitting the dataset into training and validation sets for machine learning models. The training set is used to train the model, while the validation set helps evaluate its performance. The 75:25 ratio used strikes a balance between having enough data to train the model effectively and having a substantial validation set for robust evaluation.

```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(29)

# Cross Validation Set-up
inTrain <- createDataPartition(pcos_cleaned$`PCOS`, p=.75, list = F)
train <- pcos_cleaned[inTrain,]
valid <- pcos_cleaned[-inTrain,]
```

$~$

#### Decision Tree (rpart):

The initial decision tree constructed is based on `PCOS` against the entire dataset. Employing a 75:25 cross-validation approach to divided the data I created the decision tree. The output is below:

```{r, echo=FALSE, fig.height=10, fig.width=10, fig.align='center'}
# create the decision tree
rpart_model <- rpart(`PCOS` ~ ., method = "class", data = train)

# display the decision tree
prp(rpart_model, main = "Fig. 11 - Decision Tree with entire dataset", extra=1, faclen=0,  nn=T, box.palette="Blues")
```

$~$

Then we test the model using the validation dataset. The results are seen in the confusion matrix and statistics output.

The confusion matrix shows the following layout:

* True Negative (TN) = 88: Actual class 0 (negative) correctly predicted as 0.
* False Positive (FP) = 13: Actual class 0 incorrectly predicted as 1 (positive).
* False Negative (FN) = 6: Actual class 1 incorrectly predicted as 0.
* True Positive (TP) = 28: Actual class 1 correctly predicted as 1.

Accuracy and Confidence Interval (CI):

Accuracy: 85.93% - The proportion of correctly classified instances out of the total instances.
95% Confidence Interval: The range within which the true accuracy is likely to lie.

Other Statistics:

* No Information Rate (NIR): The accuracy rate if the model simply predicted the majority class for all instances.

Interpretation:

* The model has a relatively high accuracy (85.93%), suggesting it correctly predicts classes in the dataset.
* Sensitivity (True Positive Rate) is high (93.62%), indicating that the model is good at identifying actual positive instances.
* Specificity (True Negative Rate) is moderate (68.29%), suggesting a decent ability to identify actual negative instances.
* The positive predictive value (Precision) is 87.13%, indicating the proportion of correctly predicted positive instances out of all positive predictions.

There might be a class imbalance issue, considering the differences in sensitivity and specificity. Overall, while the model shows relatively good performance, further analysis might be required to understand its behavior, especially if sensitivity or specificity is more critical for the specific application or domain.

```{r,echo=FALSE}
# creating our prediction
rpart_result <- predict(rpart_model, newdata = valid[, !colnames(valid) %in% "PCOS"], type = 'class')

# confusion matrix
confusionMatrix(rpart_result, as.factor(valid$`PCOS`))
```
$~$

```{r, echo=FALSE}
# confirming statistical predictions
dt_precision <- precision(rpart_result,as.factor(valid$`PCOS`))
dt_recall <- recall(rpart_result, as.factor(valid$`PCOS`))
dt_F <- F_meas(rpart_result, as.factor(valid$`PCOS`))
```


$~$

Upon examining the individual contributions of each variable. It's evident that `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, and `Weight_gain`, hold significant influence within the dataset. I expected a wider range of influential bloodwork-related tests beyond those presented in this outcome.

```{r, echo=FALSE}
# contribution of variables
varImp(rpart_model) %>% kable()
```
$~$

The model's accuracy stands at 85.93%, which is commendable. However, considering the variable contributions, I'm planning to build a second decision tree by removing the variables with lower contributions.

```{r, echo=FALSE}
# Extract accuracy from the confusion matrix
accuracy_rpart <- confusionMatrix(rpart_result, as.factor(valid$`PCOS`))$overall["Accuracy"]
kable(accuracy_rpart, align = "l")
```
$~$

##### Second Decision Tree:

The second decision tree is based off the variables with the highest contribution: `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, `Weight_gain` and the target variable `PCOS`. I also created a second cross validation data set that include the 6 variables chosen. The results are below:

```{r, echo=FALSE, fig.height=10, fig.width=10, fig.align='center'}
# creating the second dataset from the original
pcos_cleaned2 <- pcos_cleaned %>%
  select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)

# create some random number for reproduction
set.seed(28)

# Second Cross Validation Set-up
inTrain2 <- createDataPartition(pcos_cleaned2$`PCOS`, p=.75, list = F)
train2 <- pcos_cleaned2[inTrain2,]
valid2 <- pcos_cleaned2[-inTrain2,]

# create the second decision tree
rpart_model2 <- rpart(`PCOS` ~ ., method = "class", data = train2)

# display the decision tree
prp(rpart_model2, main = "Fig. 12 - Second Decision Tree with 6 variables", extra=1, faclen=0,  nn=T, box.palette="Blues")
```

$~$

Same as before, we create the confusion matrix and statistics for the second decision tree using the validation data:

Interpretation:

* The model displays high accuracy (91.85%) in correctly predicting classes.
* Sensitivity (True Positive Rate) is very high (98.94%), indicating an excellent ability to identify actual positive instances.
* Specificity (True Negative Rate) is moderate (75.61%), suggesting a decent ability to identify actual negative instances.
* Positive Predictive Value (Precision) is 90.29%, meaning that around 90.29% of the predicted positive instances are actually positive.

The model seems to perform well in this dataset, with high sensitivity and specificity, indicating robustness in classifying both positive and negative instances effectively.
```{r, echo=FALSE}
# creating our prediction
rpart_result2 <- predict(rpart_model2, newdata = valid2[, !colnames(valid2) %in% "PCOS"], type = 'class')

# creating the second confusion matrix
confusionMatrix(rpart_result2, as.factor(valid2$`PCOS`))
```

$~$

Let's examine the impact of each variable within the context of the second dataset. There's a noticeable shift in the overall contribution levels among the variables.

```{r, echo=FALSE}
# contribution of variables
varImp(rpart_model2) %>% kable()
```
$~$

Ultimately, we observe that the accuracy has increased to 91.85% compared to the initial decision tree.

```{r, echo=FALSE}
# Extract accuracy from the confusion matrix
accuracy_rpart2 <- confusionMatrix(rpart_result2, as.factor(valid2$`PCOS`))$overall["Accuracy"]
kable(accuracy_rpart2, align = "l")
```

$~$

#### Random Forest (randomForest):

The third model created is a random forest model for the entire dataset. Following similar steps as the decision tree, the model was trained using the entire dataset. The model's performance, as indicated by the confusion matrix, demonstrates a limited accuracy and predictive power. The Kappa statistic reflects weak agreement, and the sensitivity and specificity are relatively imbalanced. This suggests that the model might require further refinement or alternative approaches to improve its predictive capability.

```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(30)

# Cross Validation Set-up
rf_inTrain <- createDataPartition(pcos_cleaned$`PCOS`, p=.75, list = F)
rf_train <- pcos_cleaned[rf_inTrain,]
rf_valid <- pcos_cleaned[-rf_inTrain,]
```


```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(rf_train$PCOS)
levels(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in rf_train
rf_train$PCOS <- factor(rf_train$PCOS)

# Convert PCOS to factor in rf_valid
rf_valid$PCOS <- factor(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(rf_train$PCOS)
levels(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in rf_train.
rf_valid$PCOS <- factor(rf_valid$PCOS, levels = levels(rf_train$PCOS))
levels(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# # Check the length of svm_result and svm_valid$PCOS
# length_rf_result <- length(rf_result)
# length_rf_valid <- length(rf_valid$PCOS)
# 
# # Print the lengths for comparison
# print(length_rf_result)
# print(length_rf_valid)
```

```{r, echo=FALSE, error = TRUE}
#create some random number for reproduction
set.seed(39)

# create random forest model using the training data
rf_model <- randomForest(PCOS~., rf_train)
rf_model

# prediction
rf_result <- predict(rf_model, newdata = valid[, !colnames(valid) %in% "PCOS"])

# Create a confusion matrix
confusionMatrix(data = rf_result, reference = rf_valid$PCOS)
```


$~$

The plot below demonstrates the variable importance of the entire dataset. `Follicle_NoR`, `FollicleNoL` and `Hair_growth` are the top 3 variables that play a significant role in the classification of PCOS presence.
```{r, echo=FALSE, fig.height=8}
# plot for rf_model fig 13
varImpPlot(rf_model, main="Fig. 13 - Feature importance of first random forest model")
```

Numerically, we can see the same result below:
```{r, echo=FALSE}
# table for rf_model variable contribution
varImp(rf_model) %>% kable()
```
$~$

The model's accuracy stood at 62.96%, demonstrating a performance that was not particularly strong.
```{r, echo=FALSE, error=TRUE}
set.seed(321)
# Extract accuracy from the confusion matrix for the rf_model
accuracy_rf <- confusionMatrix(rf_result, valid$PCOS)$overall["Accuracy"]
accuracy_rf
```

$~$

#### Second Random Forest Model:

The second random forest model was created using the 6 variables `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, `Weight_gain` and the target variable `PCOS`. After training this model, the following confusion matrix was the result.

```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(78)

# Second RF Cross Validation Set-up
rf_inTrain2 <- createDataPartition(pcos_cleaned2$`PCOS`, p=.75, list = F)
rf_train2 <- pcos_cleaned2[rf_inTrain2,]
rf_valid2 <- pcos_cleaned2[-rf_inTrain2,]
```

```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(rf_train2$PCOS)
levels(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in rf_train
rf_train2$PCOS <- factor(rf_train2$PCOS)

# Convert PCOS to factor in rf_valid
rf_valid2$PCOS <- factor(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(rf_train2$PCOS)
levels(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in rf_train.
rf_valid2$PCOS <- factor(rf_valid2$PCOS, levels = levels(rf_train2$PCOS))
levels(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# # Check the length of rf_result and rf_valid$PCOS
# length_rf_result2 <- length(rf_result2)
# length_rf_valid2 <- length(rf_valid2$PCOS)
# 
# # Print the lengths for comparison
# print(length_rf_result2)
# print(length_rf_valid2)
```


```{r, echo=FALSE}
# create some random number for reproduction
set.seed(7)

# create the second random forest model using the training data from the third decision tree
rf_model2 <- randomForest(PCOS ~ Follicle_NoR + Follicle_NoL + Weight_gain + Skin_darkening + Hair_growth, data = rf_train2)
rf_model2

# creating the prediction for the third decision tree
rf_result2 <- predict(rf_model2, newdata = rf_valid2[, !colnames(rf_valid2) %in% "PCOS"])

# Convert PCOS column to factor in rf_train2 and rf_valid2
rf_train2$PCOS <- factor(rf_train2$PCOS)
rf_valid2$PCOS <- factor(rf_valid2$PCOS)

# # Check unique levels in rf_result2 and rf_valid2$PCOS
# unique_levels_result <- unique(rf_result2)
# unique_levels_valid <- unique(rf_valid2$PCOS)
# 
# # Check if the levels match
# identical(unique_levels_result, unique_levels_valid)
# 
# # If levels do not match, manually set levels in rf_result2 to match those in rf_valid2$PCOS
# levels(rf_result2) <- levels(rf_valid2$PCOS)
# 
# Convert rf_result2 to factor and align levels with rf_valid2$PCOS
rf_result2_factor <- factor(rf_result2, levels = levels(rf_valid2$PCOS))

# Create a confusion matrix
confusionMatrix(data = rf_result2_factor, reference = rf_valid2$PCOS)
```

$~$

From the random forest model we created, we can create a variable importance plot which shows each variable and how important it is in classifying the data. From the plot below we note that `Follicle_NoR` and `Follicle_NoL` are among the top variables that play a significant role in the classification of having or not having PCOS .

```{r, echo=FALSE}
# plot for the second rf_model fig 14
varImpPlot(rf_model2, main="Fig. 14 - Feature importance of second random forest model")
```

$~$

Numerically, we can see the same result below:
```{r, echo=FALSE}
# table for rf_model2 variable contribution
varImp(rf_model2) %>% kable()
```

$~$

Lastly, I checked against the validation data the accuracy of the second model with the results of 90.37% accuracy which surpasses the first random forest model.

```{r, echo=FALSE}
# Extract accuracy from the confusion matrix for the rf_model2
accuracy_rf2 <- confusionMatrix(data = rf_result2_factor, reference = rf_valid2$PCOS)$overall["Accuracy"]
accuracy_rf2
```

$~$

#### Gradient Boosting Machines (xgboost or gbm):

The fifth model is the start of the Gradient Boosting Machines model which is created using the entire dataset with the target variable `PCOS`. 

In a GBM model, the "rel.inf" typically denotes the relative importance of each feature in predicting the target variable. `Follicle_NoR` is identified as the most crucial variable in the model, followed by others that contribute to a lesser extent.

```{r, echo=FALSE}
# Set seed for reproducibility
set.seed(67)

# Train the GBM model
gbm_model <- gbm(`PCOS` ~ ., data = train, distribution = "bernoulli", n.trees = 100, interaction.depth = 4, shrinkage = 0.01, bag.fraction = 0.5)

# Predict on the validation dataset fig 15
gbm_pred <- predict(gbm_model, newdata = valid, type = "response")

# Print the summary of the trained model
summary(gbm_model, main="Fig.15 - Summary of first GBM model")
```

$~$

We create the confusion matrix and statistics for the first gbm. The model exhibits strong overall accuracy, substantial agreement (Kappa), and good performance in correctly identifying positive and negative classes, as indicated by sensitivity, specificity, and predictive values.

```{r, echo=FALSE}
# Calculate predicted classes (0 or 1) based on the predicted probabilities
predicted_classes <- ifelse(gbm_pred > 0.5, 1, 0)

# Create confusion matrix
confusionMatrix(data = factor(predicted_classes), reference = factor(valid$`PCOS`))
```

$~$

The accuracy of this model is 88.89% which is good but a second model will be created with the selected variables.
```{r, echo=FALSE}
# Calculate accuracy
gbm_accuracy <- sum(predicted_classes == valid$`PCOS`) / length(valid$`PCOS`)
cat("Accuracy:", gbm_accuracy)
```


$~$

##### Second GBM:

The second GBM is based off the variables: `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, `Weight_gain` and the target variable `PCOS`. The results are below:

Similar to before: `Follicle_NoR` is identified as the most crucial variable in the model but with the decrease rel.inf of 55.111884.

```{r, echo=FALSE}
# creating the second dataset from the original
pcos_cleaned3 <- pcos_cleaned %>%
  select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)

# Set seed for reproducibility
set.seed(68)

# Cross Validation Set-up
inTrain3 <- createDataPartition(pcos_cleaned3$`PCOS`, p=.75, list = F)
train3 <- pcos_cleaned3[inTrain3,]
valid3 <- pcos_cleaned3[-inTrain3,]

# Train the GBM model
gbm_model2 <- gbm(`PCOS` ~ ., data = train3, distribution = "bernoulli", n.trees = 100, interaction.depth = 4, shrinkage = 0.01, bag.fraction = 0.5)

# Predict on the validation dataset fig 16
gbm_pred2 <- predict(gbm_model2, newdata = valid3, type = "response")

# Print the summary of the trained model
summary(gbm_model2, main="Fig.16 - Summary of second GBM model")
```

$~$

The model also exhibits a reasonably good accuracy and agreement between actual and predicted classes. It shows higher sensitivity than the first model but relatively lower specificity. The model's overall performance is moderately robust but might have some limitations in correctly identifying negative instances (class 0).

```{r, echo=FALSE}
# Calculate predicted classes (0 or 1) based on the predicted probabilities
predicted_classes2 <- ifelse(gbm_pred2 > 0.5, 1, 0)

# Create confusion matrix
confusionMatrix(data = factor(predicted_classes2), reference = factor(valid3$`PCOS`))
```

$~$

The accuracy observed in this GBM model is slightly reduced compared to the initial one, achieving an 88.15% accuracy rate.
```{r, echo=FALSE}
# Calculate accuracy
gbm_accuracy2 <- sum(predicted_classes2 == valid3$`PCOS`) / length(valid3$`PCOS`)
cat("Accuracy:", gbm_accuracy2)
```


$~$

#### Support Vector Machines (e1071):

To start the SVM algorithm and following the similar criteria as the previous models by setting up the cross validation set-up, create the prediction, confusion matrix and lastly it's accuracy.

```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(train$PCOS)
levels(valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in svm_train
train$PCOS <- factor(train$PCOS)

# Convert PCOS to factor in svm_valid
valid$PCOS <- factor(valid$PCOS)
```


```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(train$PCOS)
levels(valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# checking the structure of both valid and train datasets
str(valid)
str(train)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in svm_train.
valid$PCOS <- factor(valid$PCOS, levels = levels(train$PCOS))
levels(valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# # Check the length of svm_result and svm_valid$PCOS
# length_svm_result <- length(svm_result)
# length_svm_valid <- length(svm_valid$PCOS)
# 
# # Print the lengths for comparison
# print(length_svm_result)
# print(length_svm_valid)
```

$~$

The confusion matrix below shows sensitivity (true positive rate) for class 0 is high at 95.74%, indicating the model's effectiveness in identifying instances of class 0. Specificity for class 1 is 80.49%, which is reasonably good. The balanced accuracy, which considers both sensitivity and specificity, is at 88.12%, demonstrating a good overall performance in distinguishing between both classes. Overall, based on these metrics, the confusion matrix generally portrays a good performance of the model.

```{r, echo=FALSE, error = TRUE }
#create some random numbers for reproduction
set.seed(31)

# SVM
svm_model <- svm(PCOS ~ ., train)

# create prediction
svm_result <- predict(svm_model, newdata = valid)


# confusion matrix for svm
confusionMatrix(svm_result, valid$PCOS)
```

```{r, echo=FALSE, error=TRUE}
#plot support vector machine
plot(svm_model, train)
```


$~$

Within the SVM, there were 98 instances have been predicted as class 0 and 37 instances have been predicted as class 1.
```{r, echo=FALSE, error=TRUE}
# summary of svm_result
summary(svm_result)
```

$~$

The accuracy for the first model is 91.11%.
```{r, echo=FALSE, error=TRUE}
#Extract accuracy from the confusion matrix
accuracy_svm <- confusionMatrix(svm_result, as.factor(valid$`PCOS`))$overall["Accuracy"]
accuracy_svm
```

$~$

##### Second SVM:

The second SVM model was constructed following the same methodology as the initial model, utilizing the identical selection of the six variables employed thus far.

```{r, echo=FALSE}
# create some random numbers for reproduction
set.seed(8)

# Cross Validation Set-up
svm_inTrain2 <- createDataPartition(pcos_cleaned2$PCOS, p=.75, list = FALSE)
svm_train2 <- pcos_cleaned2[svm_inTrain2,]
svm_valid2 <- pcos_cleaned2[-svm_inTrain2,]
```

```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(svm_train2$PCOS)
levels(svm_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in svm_train2
svm_train2$PCOS <- factor(svm_train2$PCOS)

# Convert PCOS to factor in svm_valid2
svm_valid2$PCOS <- factor(svm_valid2$PCOS)
```


```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(svm_train2$PCOS)
levels(svm_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in svm_train2
valid$PCOS <- factor(svm_valid2$PCOS, levels = levels(svm_train2$PCOS))
levels(svm_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# # Check the length of svm_result and svm_valid$PCOS
# length_svm_result <- length(svm_result)
# length_svm_valid2 <- length(svm_valid2$PCOS)
# 
# # Print the lengths for comparison
# print(length_svm_result)
# print(length_svm_valid2)
```

$~$

The confusion matrix for the second SVM suggest that the model performs well in classifying instances, demonstrating high accuracy, sensitivity, specificity, and precision for the predicted classes. The Kappa statistic also indicates substantial agreement between the actual and predicted classes.
```{r, echo=FALSE, error = TRUE}
# Second SVM
svm_model2 <- svm(PCOS ~ Follicle_NoR + Follicle_NoL + Weight_gain + Skin_darkening + Hair_growth, svm_train2)

# create prediction
svm_result2 <- predict(svm_model2, newdata = svm_valid2)

# confusion matrix for svm_valid2
confusionMatrix(svm_result2, svm_valid2$PCOS)
```


```{r, echo=FALSE, error=TRUE}
#plot second support vector machine model
plot(svm_model2, svm_train2)
```



```{r, echo=FALSE, error=TRUE}
# summary
summary(svm_result2)
```

$~$

The second model demonstrated enhanced performance, achieving an accuracy of 91.85%.
```{r, echo=FALSE, error=TRUE}
#Extract accuracy from the confusion matrix
accuracy_svm2 <- confusionMatrix(svm_result2, svm_valid2$`PCOS`)$overall["Accuracy"]
accuracy_svm2
```


$~$

#### Neural Networks (neuralnet):

Utilizing the `neuralnet` package in R I created the neural network models. The neural network is constructed to predict the target variable `PCOS` using all available predictors. The model is then trained using the training dataset using hidden argument that help specify the architecture of the neural network. These included setting up two hidden layers with 12 and 8 neurons, respectively. The stepmax parameter determines the maximum number of iterations allowed during training, here set to 20000 to ensure the model has enough iterations to learn from the data and optimize its parameters.

For context, neural network plots are composed of:

* The left-most nodes (i.e. input nodes) are the raw data variables used in the model.

* The arrows in black (and associated numbers) are the weights which you can think of as how much that variable contributes to the next node. The blue lines are the bias weights which are an additional parameter introduced to each neuron and serves as an offset or intercept term, contributing to the ability of the network to fit more complex patterns in the data.

* The middle nodes (i.e. anything between the input and output nodes) are your hidden nodes. This is where the image analogy helps. Each of these nodes constitute a component that the network is learning to recognize. 

* The far-right (output node(s)) node is the final output of the neural network.


```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(67)

# Cross Validation Set-up
nn_inTrain <- createDataPartition(pcos_cleaned$PCOS, p=.75, list = F)
nn_train <- pcos_cleaned[nn_inTrain,]
nn_valid <- pcos_cleaned[-nn_inTrain,]
```

$~$

```{r, echo=FALSE, error=TRUE}
# set a seed for reproducibility purposes
set.seed(19)

# create the model
nn_model <- neuralnet(`PCOS`~.,
                      data = nn_train,
                      hidden = c(12, 8),  # Specify the number of hidden layers and neurons
                      linear.output = FALSE,
                      stepmax = 20000  # Increase the maximum number of iterations
)
```

Although not displayed when knitting, the plot produced is seen below:

![Fig. 17 - Neural Network 1](/Users/letisalba/Desktop/Data-698/Final-Presentation-and-Paper/figs:tables/fig17.png)

```{r, echo=FALSE, fig.width=18, fig.height=12, fig.align='center', include=FALSE, error=TRUE}
# create the plot based on the model above
#plot(nn_model, rep = "best", main="")
#grid::grid.text("Fig. 15 - Neural Network", x = 0.5, y = 0.1)
```

```{r, echo=FALSE, error=TRUE}
# make predictions on the test data using a previously trained model
pred <- predict(nn_model, valid)

# create a vector of labels for the two possible `PCOS(Y/N)` status in the dataset.
labels <- c("0", "1")

# creates a data frame with the column index of the maximum value in each row of the "pred" variable
prediction_label <- data.frame(max.col(pred)) %>%
# use the mutate function to add a new column to the data frame called "pred"
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
# convert the data frame to a vector.
unlist()

# print the table
table(valid$`PCOS`, prediction_label)
```

$~$

The accuracy of the model above was of 30.37%. Hoping this could improve in the second model.

```{r, echo=FALSE, error=TRUE}
#checking the accuracy
check <- as.numeric(valid$`PCOS`) == max.col(pred)
nn_accuracy <-  (sum(check)/nrow(valid))
nn_accuracy
```

$~$

##### Second Neural Network:

For the second neural network model I am also using the `neuralnet` package. Similar to the first model, I created the model by specifying the variables from `pcos_cleaned2` which included `Follicle_No(L)`, `Follicle_No(R)`, `Hair_growth`, `Skin_darkening`, `Weight_gain`, and the target variable `PCOS` and setting the parameters to 1 hidden layer containing 2 neurons and an output layer with 1 neuron.

```{r, echo=FALSE}
# set a seed for reproducibility purposes
set.seed(13)

# create the second model
nn_model2 <-  neuralnet(`PCOS`~Follicle_NoL + Follicle_NoR + Hair_growth + Skin_darkening + Weight_gain,
                        data=train2,
                        hidden=c(2,1),
                        linear.output = FALSE,
                        stepmax = 10000  # Increase the maximum number of iterations
)
```

$~$

The plot below is the output of the model created:

In the context of a `neuralnet` plot displaying an error of 13.733636 and steps of 2518, this information typically pertains to the training progress of a neural network model and provides insights into the model's learning behavior.

* An error value of 13.733636 suggests the current level of model error or loss at the 2518th step/iteration during training.

* Each step involves adjusting the model's weights and biases to reduce the error, moving closer towards an optimal solution. Higher step counts usually mean more training iterations were required for the network to converge.

```{r, echo=FALSE}
# create the plot based on the model above
plot(nn_model2, rep = "best", main="")
grid::grid.text("Fig. 18 -  Second Neural Network", x = .5, y = .2)
```

$~$

Below is a table displaying the predicted outcomes for the `PCOS` status, where the highest count of predictions corresponds to the absence of PCOS.

```{r, echo=FALSE}
# make predictions on the test data using a previously trained model
pred2 <- predict(nn_model2, valid2)

# create a vector of labels for the two possible `PCOS` status in the dataset.
labels2 <- c("0", "1")

# creates a data frame with the column index of the maximum value in each row of the "pred" variable
prediction_label2 <- data.frame(max.col(pred2)) %>% 
# use the mutate function to add a new column to the data frame called "pred"
mutate(pred=labels2[max.col.pred2.]) %>%
select(2) %>%
# convert the data frame to a vector.
unlist()

# print the table
table(valid2$`PCOS`, prediction_label2)
```

$~$

Lastly, we check the accuracy using the validation data by first converting actual categorical values into numerical ones and compare them with predicted values. The `neuralnet` accuracy is 30.37% which is oddly similar to the accuracy of the first model.
```{r, echo=FALSE}
# checking the accuracy
check2 <- as.numeric(valid2$`PCOS`) == max.col(pred2)
nn_accuracy2 <-  (sum(check2)/nrow(valid2))
nn_accuracy2
```


$~$

#### K-Nearest Neighbors (kknn or class):

To build the k-Nearest Neighbors model I prepared the data by removing rows with missing values, although there were none, the code wouldn't run without it. After setting a seed for reproducibility I trained a kNN model using the training data and predict the 'PCOS' variable for the validation dataset. The kNN algorithm allows tweaking the number of neighbors (k) to impact the outcome. Here, it was fixed at 5; as a result, the accuracy achieved was 65.19%.

```{r, echo=FALSE}
# Remove rows with missing values from train and valid datasets
train <- train[complete.cases(train), ]
valid <- valid[complete.cases(valid), ]

# set a seed for reproducibility purposes
set.seed(78)

# Set the value of k for kNN
k <- 5  # Change this value as needed

# Fit the kNN model using the training data
knn_model <- knn(train[, -which(names(train) == "PCOS")], 
                 valid[, -which(names(valid) == "PCOS")], 
                 train$`PCOS`, 
                 k = k)
```


```{r, echo=FALSE}
# Create confusion matrix for knn model
conf_matrix <- confusionMatrix(knn_model, valid$`PCOS`)
conf_matrix

# Plot confusion matrix
plot(conf_matrix$table, col = conf_matrix$byClass, 
     main = "Fig. 19 - Confusion Matrix for first kNN Model",
     xlab = "Predicted",
     ylab = "Actual")
```


```{r, echo=FALSE}
# Calculate accuracy
knn_accuracy <- mean(knn_model == valid$`PCOS`)
knn_accuracy
```

$~$

#### Second kNN:

For the second kNN, the same steps were takes as the first model with the exception of using the 6 variables of `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth` and target variable `PCOS`. The result, an accuracy of 88.15% was achieved, which is much higher than the first model.

```{r, echo=FALSE}
# Filter and select the desired columns for the new dataset
pcos_cleaned4 <- pcos_cleaned %>%
  select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, 
         `Skin_darkening`, `Hair_growth`)

# Split the data into training and validation sets (if needed)
set.seed(123)  # Set seed for reproducibility
inTrain4 <- createDataPartition(pcos_cleaned4$`PCOS`, p = 0.75, list = FALSE)
train4 <- pcos_cleaned4[inTrain4, ]
valid4 <- pcos_cleaned4[-inTrain4, ]

# Check for missing values and remove them if present
train4 <- train4[complete.cases(train4), ]
valid4 <- valid4[complete.cases(valid4), ]

# Set the value of k for kNN
k <- 5  # Change this value as needed

# Fit the kNN model using the training data
knn_model2 <- knn(train4[, -which(names(train4) == "PCOS")], 
                  valid4[, -which(names(valid4) == "PCOS")], 
                  train4$`PCOS`, 
                  k = k)
```


```{r, echo=FALSE}
# Create confusion matrix for the second model
conf_matrix2 <- confusionMatrix(knn_model2, valid$`PCOS`)
conf_matrix2

# Plot confusion matrix
plot(conf_matrix2$table, col = conf_matrix2$byClass, 
     main = "Fig. 20 - Confusion Matrix for second kNN Model",
     xlab = "Predicted",
     ylab = "Actual")
```


```{r, echo=FALSE}
# Calculate accuracy for the new kNN model
knn_accuracy2 <- mean(knn_model2 == valid4$`PCOS`)
knn_accuracy2
```

$~$

### Model Comparison:

Lastly, let's do a model comparison. Overall the top 5 models with the highest accuracies were: Decision Tree (second model), SVM (second and first model), Random Forest (second model), and Gradient Boost Machine (first model).

Decision Tree 2 and SVM 2: Both Decision Tree 2 and SVM 2 achieved similar high accuracies of approximately 91.85%. This implies that these models were quite successful in making accurate predictions for PCOS diagnosis based on the given data and features.

SVM 1:SVM 1 achieved a slightly lower accuracy compared to Decision Tree 2 and SVM 2 but still performed reasonably well at approximately 91.11%. It remains effective in predicting PCOS diagnosis but might be slightly less accurate than the aforementioned models.

Random Forest 2: The Random Forest model achieved an accuracy of around 90.37%, indicating its capability to correctly classify PCOS and non-PCOS cases with slightly less accuracy than SVM 1 and Decision Tree 2.

Gradient Boost Machines 1 and k-Nearest 2: Both Gradient Boost Machines 1 and k-Nearest 2 models obtained accuracies around 88.15%, which, while lower than the previous models, still demonstrate a moderate ability to predict PCOS diagnosis based on the provided features.

Overall, these accuracies suggest that Decision Tree 2, SVM 2, SVM 1, and Random Forest 2 performed relatively better in distinguishing between PCOS and non-PCOS cases in this dataset, while Gradient Boost Machines 1 and k-Nearest 2 showed somewhat lower but still reasonably acceptable prediction accuracies.


```{r, echo=FALSE}
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", 
                 "Random Forest 1", "Random Forest 2", 
                 "Gradient Boost Machines 1", 
                 "Gradient Boost Machines 2",
                 "SVM 1", "SVM 2", "Neural Network 1", 
                 "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 
                0.6296296, 0.9037037, 
                0.8814815, 0.6222222, 
                0.9111111, 0.9185185, 
                0.3037037, 0.3037037, 
                0.6518519, 0.8814815)

# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)

# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]

# Display the results
kable(results, caption = "<font color=#000000><b>Table 5. </b>Model Comparison </font>", format = "html") %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

### Conclusion:

Initially, I presumed that due to the scarcity of information within the medical domain and the limited availability of datasets for analysis, I held reservations about the potential success in accurately predicting a PCOS diagnosis. Surprisingly, 8 out of the 12 models developed exhibited accuracies surpassing 85%, with only 4 out of the 8 achieving an accuracy exceeding 90%. In comparison to the findings in existing literature utilizing the same Kaggle dataset, the top 5 algorithms I employed showed a slightly lower performance, where these achieved accuracies above 95%. If I had to pick an algorithm to best represent my dataset I'd select `SVM 1` as its accuracy was of 91.11%, a unique percentage not replicated by other algorithms. Conversely, the utilization of Neural Networks in this analysis did not yield satisfactory results, deviating from the expectations set by the literature. This outcome might be attributed to the specific package utilized and the parameter settings chosen for the neural network model. For the future work, I aim to explore datasets such as [NICHDash](https://dash.nichd.nih.gov/study/15920) which are not publicly available and feature a slightly larger and more diverse population within the United States. This exploration will facilitate a more comprehensive assessment of PCOS, enabling a deeper understanding of the condition's nuances and complexities.

Having subjected the data to various algorithms, I am now able to address the anticipated questions:

1.	Are there commonalities women with and without PCOS have that can be easily dismissed as normal?

  * Some shared patterns identified through analysis included Body Mass Index (BMI), vital signs, biometric measurements, and specific hormone levels, yet within this dataset, insufficient evidence exists to conclusively dismiss these as unrelated to Polycystic Ovary Syndrome (PCOS).

2.	Are there differences for women of different race/ethnic background when it comes to having PCOS? What about women without PCOS?

  * These accuracies don't directly address racial or ethnic differences as the dataset only involved women from Kerala, India. Further analysis involving feature exploration or specific subgroup analysis using demographic data might unveil associations or variations among different racial/ethnic groups in PCOS prevalence or features.

3.	What is the likelihood of a woman developing PCOS based on her age, ethnicity, and BMI history?

  * The accuracies did not explicitly signify predictions regarding probability due to the lack of diverse ethnic backgrounds in the collected records. Moreover, age and BMI were not influential variables across most of the created models in this dataset. Specific models with feature importance or coefficients might offer insights into how age, ethnicity, and BMI contribute to predicting PCOS likelihood but the data would have to include such diversity.

4.	Can we predict the risk of insulin resistance, diabetes, and cardiovascular disease in women with PCOS based on their medical history, hormone levels, and lifestyle factors?

  * Machine learning models could help predict the risk of insulin resistance, diabetes, and cardiovascular disease in women with PCOS based on available medical history, hormone levels, and lifestyle factors. For this a larger dataset would be needed in order to create specialized modeling to derive precise predictions. Regrettably, my dataset would not be well-suited for conducting such forecasts.

5.	Can we predict the likelihood of successful pregnancy outcomes in women with PCOS based on their age, weight, hormone levels, and treatment history?

  * Similar to the above, machine learning models can potentially predict the likelihood of successful pregnancy outcomes in women with PCOS based on various factors like age, weight, hormone levels, and treatment history. A more expansive dataset, specialized models, or in-depth analyses could offer more intricate predictive insights, exceeding the capabilities of my current dataset.

6.	Can we predict the long-term health outcomes and quality of life of women with PCOS based on their age, lifestyle factors, hormone levels, and treatment history?

  * Machine learning models, when trained with extensive data including age, lifestyle factors, hormone levels, and treatment history, might offer predictive insights into long-term health outcomes and quality of life for women with PCOS. However, these models might need additional feature engineering and specialized analyses to offer accurate predictions. I expect that accurately predicting long-term health outcomes, despite an improved dataset, will remain challenging due to the varied presentation of PCOS in women, which poses ongoing diagnostic challenges.

$~$

### References:

1.	Aggarwal, S., & Pandey, K. (2023). Early identification of PCOS with commonly known diseases: Obesity, diabetes, high blood pressure, and heart disease using machine learning techniques. Expert Systems with Applications, 217, 119532. https://doi.org/10.1016/j.eswa.2023.119532

2.	Anda, D., & Iyamah, E. (2022, December). Comparative analysis of artificial intelligence in the diagnosis of ... ResearchGate. Retrieved April 1, 2023, from https://www.researchgate.net/publication/366320486_Comparative_Analysis_of_Artificial_Intelligence_in_the_Diagnosis_of_Polycystic_Ovary_Syndrome 

3.	Bartlett, E., & Erlich, L. (2015). Part 3: Dealing with Obstacles — Chapter 5: Polycystic Ovary Syndrome (PCOS). In Feed your fertility: Your guide to cultivating a healthy pregnancy with traditional Chinese medicine, real food, and holistic living (pp. 342–349). essay, Fair Winds Press. 

4.	Bulsara, J., Patel, P., Soni, A., &amp; Acharya, S. (2021, February 10). A review: Brief insight into polycystic ovarian syndrome. Endocrine and Metabolic Science. Retrieved February 23, 2023, from https://www.sciencedirect.com 

5. Engmann, L., Jin, S., Sun, F., Legro, R. S., Polotsky, A. J., Hansen, K. R., Coutifaris, C., Diamond, M. P., Eisenberg, E., Zhang, H., Santoro, N., & Reproductive Medicine Network (2017). Racial and ethnic differences in the polycystic ovary syndrome metabolic phenotype. American journal of obstetrics and gynecology, 216(5), 493.e1–493.e13. https://doi.org

6.	Goodarzi, Carmina, E., & Azziz, R. (2015). DHEA, DHEAS and PCOS. The Journal of Steroid Biochemistry and Molecular Biology, 145, 213–225. https://doi.org/10.1016/j.jsbmb.2014.06.003

7.	Hassan, Malik & Mirza, Tabasum. (2020). Comparative Analysis of Machine Learning Algorithms in Diagnosis of Polycystic Ovarian Syndrome. International Journal of Computer Applications. Volume 175. 10.5120/ijca2020920688.

8.	Khan, M. J., Ullah, A., & Basit, S. (2019). Genetic Basis of Polycystic Ovary Syndrome (PCOS): Current Perspectives. The application of clinical genetics, 12, 249–260. https://doi.org/

9.	Kambale, T., Sawaimul, K. D., & Prakash, S. (2023). A study of hormonal and anthropometric parameters in polycystic ovarian syndrome. Annals of African medicine, 22(1), 112–116. https://doi.org/10.4103/aam.aam_15_22

10.	Kavitha, K., Tangudu, N., Sahu, S. R., Narayana, G. V. L., & Anusha, V. (2023). Detection of PCOS using Machine Learning Algorithms with Grid Search CV Optimization. International Journal of Engineering Trends and Technology, 71(7), 201-208. https://doi.org/10.14445/22315381/IJETT-V71I7P219

11. Kottarathil, P. (2020). Polycystic Ovary Syndrome (PCOS) Dataset. Kaggle. https://www.kaggle.com/datasets/prasoonkottarathil/polycystic-ovary-syndrome-pcos/data

12.	MarchofDimes. (2022). Population of women 15-44 years by age: United States, 2020. March of Dimes | PeriStats. Retrieved February 24, 2023, from https://www.marchofdimes.org

13.	Patel, J., & Rai, S. (2018, September). Polycystic ovarian syndrome (PCOS) awareness among young women of central India. ResearchGate. Retrieved March 29, 2023, from https://www.researchgate.net/publication/327566794_Polycystic_ovarian_syndrome_PCOS_awareness_among_young_women_of_central_India 

14.	Ramanand, S. J., Ghongane, B. B., Ramanand, J. B., Patwardhan, M. H., Ghanghas, R. R., & Jain, S. S. (2013, January). Clinical characteristics of polycystic ovary syndrome in Indian women. Indian journal of endocrinology and metabolism. Retrieved March 18, 2023, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3659881/ 

15.	S;, K. M. J. U. A. B. (2019). Genetic basis of polycystic ovary syndrome (PCOS): Current Perspectives. The application of clinical genetics. Retrieved March 18, 2023, from https://pubmed.ncbi.nlm.nih.gov/31920361/ 

16.	S. Nasim, M. S. Almutairi, K. Munir, A. Raza and F. Younas, "A Novel Approach for Polycystic Ovary Syndrome Prediction Using Machine Learning in Bioinformatics," in IEEE Access, vol. 10, pp. 97610-97624, 2022, doi: 10.1109/ACCESS.2022.3205587.

17.	Shetty, Disha & Chandrasekaran, Baskaran & Singh, ArulWatson & Oliverraj, Joseph. (2017). Exercise in polycystic ovarian syndrome: An evidence-based review. Saudi Journal of Sports Medicine. 17. 123. 10.4103/sjsm.sjsm_10_17.

18.	 Tiwari, S., Kane, L., Koundal, D., Jain, A., Alhudhaif, A., Polat, K., Zaguia, A., Alenezi, F., & Althubiti, S. A. (2022). SPOSDS: A smart Polycystic Ovary Syndrome diagnostic system using machine learning. Expert Systems with Applications, 203, 117592. https://doi.org/10.1016/j.eswa.2022.117592

19.	Thakre, V., Vedpathak, S., Thakre, K., & Sonawani, S. (2020, December). PCOcare: PCOS detection and prediction using machine learning algorithms. ResearchGate. Retrieved April 1, 2023, from https://www.researchgate.net/publication/348627784_PCOcare_PCOS_Detection_and_Prediction_using_Machine_Learning_Algorithms 

20.	Thomas, Neetha. (2020). Prediction of polycystic ovarian syndrome with clinical dataset using a novel hybrid data mining classification technique. International journal of advanced research in engineering & technology. 11. 1872-1881,. 10.34218/IJARET.11.11.2020.174.

21.	Vikas, B., Anuhya, B. S., Chilla, M., & Sarangi, S. (2018). A Critical Study of Polycystic Ovarian Syndrome (PCOS) Classification Techniques. International Journal of Computational Engineering & Management, 21(4), 1. Retrieved from http://www.ijcem.org/volume21/issue4/IJCEM_2104_01.pdf

22.	Wijeyaratne, C. N., Seneviratne, R.deA., Dahanayake, S., Kumarapeli, V., Palipane, E., Kuruppu, N., Yapa, C., Seneviratne, R.deA., & Balen, A. H. (2011). Phenotype and metabolic profile of South Asian women with polycystic ovary syndrome (PCOS): results of a large database from a specialist Endocrine Clinic. Human reproduction (Oxford, England), 26(1), 202–213. https://doi.org/10.1093/humrep/deq310

23.	World female population, 1960-2022. Knoema. (2022). Retrieved February 24, 2023, from https://knoema.com.

24. World Health Organization. (2023, June 28). Polycystic ovary syndrome (PCOS) - Fact sheet. Retrieved July 20, 2023, from https://www.who.int/news-room/fact-sheets/detail/polycystic-ovary-syndrome?gclid=Cj0KCQiAkeSsBhDUARIsAK3tiec_ZvLzUc6iwWqRVVN2FctmFAz4u1kL-_NJysQTPNT6cjuvWJxjuUoaAhh9EALw_wcB

25.	Wu, J., Chen, X.-Y., Zhang, H., Xiong, L.-D., Lei, H., & Deng, S.-H. (2019). Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization. Journal of Electronic Science and Technology, 17(1), 26-40. https://doi.org/10.11989/JEST.1674-862X.80904120

