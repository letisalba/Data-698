---
title: "Data 698 - Final Project"
author: "Leticia Salazar"
date: "May 17, 2023"
output:
  html_document:
    theme: united
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$~$

## [Polycystic Ovarian Syndrome (PCOS)]()

$~$

## Overview: 

Polycystic Ovary Syndrome (PCOS) stands as one of the most prevalent endocrine disorders affecting reproductive-aged women globally. This complex condition disrupts hormone levels within the body, impacting the ovaries and often leading to a myriad of symptoms. PCOS is characterized by a spectrum of irregularities, including menstrual irregularities, hormonal imbalances, and the formation of cysts on the ovaries. Its manifestation varies among individuals, typically marked by excessive production of androgens, irregular menstrual cycles, and difficulties in ovulation, potentially leading to fertility challenges. Beyond reproductive health concerns, PCOS also presents with metabolic implications, such as insulin resistance, which can elevate the risk of developing type 2 diabetes and cardiovascular issues. The exact cause of PCOS remains multifaceted and isn't singularly defined, encompassing a combination of genetic, hormonal, and lifestyle factors. As a chronic disorder, PCOS poses significant challenges to those affected, impacting their overall health and quality of life.

This syndrome's diagnosis is often challenging due to its diverse array of symptoms and the absence of a singular definitive test. Clinicians typically rely on a set of diagnostic criteria, such as irregular menstrual cycles, elevated androgen levels, and the presence of ovarian cysts, to identify and evaluate PCOS. However, the complexity of this condition extends beyond its diagnostic challenges, as managing PCOS necessitates a multifaceted approach. Treatment strategies often focus on alleviating specific symptoms, such as irregular periods, infertility, and excessive hair growth, through medications, lifestyle modifications, and sometimes surgical interventions. Additionally, lifestyle changes involving diet adjustments and regular exercise play a crucial role in managing the metabolic aspects of PCOS. Research into PCOS continues to expand, aiming to enhance diagnostic accuracy, refine treatment approaches, and deepen our understanding of its underlying mechanisms to better support individuals grappling with this pervasive syndrome.

Hormones that are involved in PCOS are:

* **Androgens: ** aka "male hormones" are present in women with PCOS at higher levels than usual. Excess in androgens can cause symptoms such as acne, unwanted hair, thinning hair, and irregular periods.

* **Insulin: ** allows the body to absorb glucose (blood sugar) into the cells for energy. In PCOS, the body doesn't respond to insulin as intended therefore, elevations in blood glucose levels can be assessed. Such elevations then lead to increased production of androgen.

* **Progesterone: ** vital hormone for menstruation and pregnancy; lack of progesterone contributes to irregular periods.

$~$

PCOS Symptoms:

Many of these symptoms can be attributed to other causes or go unnoticed but it is very common for PCOS to go undiagnosed for some time. Here are some symptoms that help with the diagnosis:

* **Irregular periods: ** irregular or missed periods as are a result of not ovulating is a common signs of PCOS

* **Polycystic ovaries: ** some may develop cysts in their ovaries but some don't. Ovaries may be enlarged and follicles surrounding their eggs therefore failing to function regularly.

* **Excess androgen: ** elevated levels of male hormones can cause excess hair and acne.

$~$

Other symptoms may include:

* **Weight gain: ** many people with PCOS will have weight gain or obesity that is difficult to manage.

* **Fatigue: ** increase in fatigue or low energy is also common

* **Unwanted hair growth: ** due to excess androgen, areas such as face, arms, back, chest, hand, toes and abdomen may have excess hair growth.

* **Thinning hair on the head: ** hair loss may increase in middle age for those with PCOS

* **Infertility: ** PCOS is a leading cause for infertility but not everyone is the same.

* **Acne: ** due to hormonal changes, acne can be arise and make skin oilier than usual and cause breakout in the face, chest and upper back.

* **Darkening of skin: ** areas such as under arms, breasts or back of your neck may get dark, patchy or thicken

* **Mood changes: ** mood swings, depression and anxiety can increase

* **Pelvic pain: ** pain may occur with periods along with heavy bleeding or without bleeding

* **Headaches: ** can occur due to hormonal changes

* **Sleep problems: ** most people often suffer with problem such as insomnia or poor sleep. These arise due to many factors but a common one is having sleep apnea (sleep disorder). Even when you fall asleep you wake up as if you have not slept at all or have trouble falling asleep.

* **Depression: ** can arise due to symptoms that can alter your appearance and have a negative impact on your emotions.

**It's good to note that not everyone who is diagnosed with PCOS experiences all of these symptoms and should always consult with a their PCP or OBGYN to get an accurate diagnosis.**

$~$

## The Problem:

Early identification of risk factors associated with PCOS can assist in timely interventions and lifestyle adjustments. Tailoring suggestions or treatments based on individualized risk profiles has the potential to enhance patient outcomes. Employing predictive models can play a pivotal role in increasing awareness regarding PCOS risk factors and preventive measures. However, limitations may exist in accessing comprehensive and varied datasets containing accurate demographic, clinical, and lifestyle data. It is crucial to ensure the model's transparency and interpretability to facilitate well-informed decisions and recommendations. Additionally, it's imperative that the model demonstrates proficiency across diverse demographic groups and populations. Addressing these challenges involves the development of a robust predictive model using machine learning techniques. Such an endeavor holds the promise of significantly contributing to the identification of individuals at risk of PCOS, thereby enabling early interventions and guiding personalized healthcare strategies for improved management of the condition. This project will investigate publicly available datasets that will be used to develop machine learning models that predicts the probability or risk of an individual having or developing PCOS based on demographic information (age, ethnicity, geographical location), clinical data (hormonal levels, BMI, menstrual irregularities), and lifestyle factors (dietary habits, exercise routine, stress levels).


$~$

## The Data:

The Polycystic ovary syndrome (PCOS) dataset, available on [Kaggle.com](https://www.kaggle.com/datasets/prasoonkottarathil/polycystic-ovary-syndrome-pcos), is comprised of two csv files labeled `PCOS_data_without_infertility` and `PCOS_infertility`. In total, these files encompass 48 variables and 541 data entries all collected from 10 different hospitals across Kerala, India. The dataset contains all physical and clinical parameters to determine PCOS and infertility related issues.

**Full description of the variables below:**

* Units used range from imperial to metric system of measurement

* For Yes | No questions
  * Yes = 1
  * No = 0

Variables                               Description
----------------------------            ---------------------------------------
"Sl..No"                                unique identification number assigned to each entry
"Patient.File.No."                      file number for each patient's record.       
"PCOS..Y.N."                            indicates the presence or absence of PCOS, with "Y" denoting "1 or Yes" and "N" for "0 or No."
"I...beta.HCG.mIU.mL."                  pregnancy hormone case I measured in milli-international units per liter (mIU/L)
"II....beta.HCG.mIU.mL."                pregnancy hormone case II measured in milli-international units per liter (mIU/L)
"AMH.ng.mL."                            detects ovarian reserve (egg count)           
"Age..yrs."                             age of patient in years            
"Weight..Kg."                           weight of patient in kg            
"Height.Cm."                            height of patient in cm            
"BMI"                                   body mass index                    
"Blood.Group"                           Blood Groups: A+ = 11, A- = 12, B+ = 13, B- = 14, O+ = 15, O- = 16, AB+ = 17, AB- = 18            
"Pulse.rate.bpm."                       beats per minute       
"RR..breaths.min."                      respiration rates per minute       
"Hb.g.dl."                              hemoglobin concentration measured in grams per deciliter (g/dL).               
"Cycle.R.I."                            cycle Regularity Index used to assess the regularity or irregularity of menstrual cycles in women: 4 indicates irregular menstrual cycle, 2 indicates a regular menstrual cycle           
"Cycle.length.days."                    length of menstrual cycle     
"Marraige.Status..Yrs."                 years married  
"Pregnant.Y.N."                         pregnant yes or no         
"No..of.aborptions"                     number of abortions      
"FSH.mIU.mL."                           follicle stimulating hormone measured in milli-international units per liter (mIU/L)            
"LH.mIU.mL."                            luteinizing hormone (increases during ovulation) measured in milli-international units per liter (mIU/L)             
"FSH.LH"                                ratio between Follicle-Stimulating Hormone (FSH) and Luteinizing Hormone (LH)                
"Hip.inch."                             measurement of hips in inches              
"Waist.inch."                           measurement of waist in inches            
"Waist.Hip.Ratio"                       ratio of measurement of waist and hip       
"TSH..mIU.L."                           thyroid stimulating hormone measured in milli-international units per liter (mIU/L)           
"AMH.ng.mL."                            Anti-MÃ¼llerian Hormone (AMH) measured in nanograms per milliliter (ng/mL); a marker used in reproductive medicine to assess ovarian reserve             
"PRL.ng.mL."                            Prolactin measured in nanograms per milliliter (ng/mL); a hormone produced by the pituitary gland           
"Vit.D3..ng.mL."                        Vitamin D3 measured in nanograms per milliliter (ng/mL); is essential for bone health, immune function, and various other bodily processes.         
"PRG.ng.mL."                            Progesterone measured in nanograms per milliliter (ng/mL); a hormone involved in the menstrual cycle, pregnancy, and maintaining the uterine lining for a developing embryo.             
"RBS.mg.dl."                            Random Blood Sugar measured in milligrams per deciliter (mg/dL); it represents the level of glucose (sugar) present in the blood at a random time, without fasting.            
"Weight.gain.Y.N."                      weight gain yes or no       
"hair.growth.Y.N."                      hair growth yes or no (hirsutism)       
"Skin.darkening..Y.N."                  darkening of skin yes or no  
"Hair.loss.Y.N."                        hair loss yes or no         
"Pimples.Y.N."                          pimples (acne) yes or no           
"Fast.food..Y.N."                       consumption of fast food yes or no    
"Reg.Exercise.Y.N."                     regularly exercise yes or no                 
"BP._Systolic..mmHg."                   systolic blood pressure measured in millimeters of mercury (mmHg).     
"BP._Diastolic..mmHg."                  diastolic blood pressure measured in millimeters of mercury (mmHg)  
"Follicle.No...L."                      number of follicles on left ovary       
"Follicle.No...R."                      number of follicles in right ovary       
"Avg..F.size..L...mm."                  average size of follicles in left ovary measured in millimeters (mm)  
"Avg..F.size..R...mm."                  average size of follicles in right ovary measured in millimeters (mm)  
"Endometrium..mm."                      size of the endometrial thickness in millimeters (mm)      

$~$

### Load Libraries:

The following libraries were utilized in this assignment:

```{r, warning=FALSE, message=FALSE, cache=FALSE, comment=FALSE}
# load libraries
library(tidyverse) # data prep
library(DataExplorer) # histograms for datasets
library(skimr) # data prep
library(rpart) # decision tree package
library(rpart.plot) # decision tree display package
library(kableExtra) # kable function for tables 
library(knitr) # kable function for table
library(tidyr) # splitting data
library(ggplot2) # graphing
library(hrbrthemes) # chart customization
library(gridExtra) # layering charts
library(stringr) # data prep
library(tidymodels) # predictions
library(corrplot) # correlation plot
library(randomForest) # for the random forest
library(caret) # confusion matrix
library("e1071") #svm
library(formattable) 
library(corrplot) # correlation plot
library(caret) # confusion matrix
library(neuralnet) # neural network
library(stats) # linear and logistic regression
library(gbm) # generalized boosted models
library(xgboost) # extreme gradient boosting
library(kknn) # weighted k-Nearest neighbors
library(jtools)  # use of summ()
library(patchwork) # ggplot2 multiplot title
library(class) # knn function
```

$~$

### Load data:

The dataset selected has been incorporated into my [GitHub](https://github.com/letisalba/Data-698/tree/master/Data-Collection-and-Analysis) and imported into R.

```{r, echo=FALSE}
# load the dataset from github
pcos <- read.csv("https://raw.githubusercontent.com/letisalba/Data-698/master/Data-Collection-and-Analysis/csv/PCOS_infertility.csv")
pcos2 <- read.csv("https://raw.githubusercontent.com/letisalba/Data-698/master/Data-Collection-and-Analysis/csv/PCOS_without_infertility.csv")
```

$~$

#### Displaying the `pcos` data:
```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}
# display the `pcos` dataset

pcos %>% 
kable(format = "html", col.names = colnames(pcos)) %>%
  kable_styling() %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```
$~$

$~$

#### Displaying the `pcos2` data:
```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}
# display the `pcos2` dataset

pcos2 %>% 
kable(format = "html", col.names = colnames(pcos2)) %>%
  kable_styling() %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

### Data Exploration:

Using the `skimr` library we can obtain a quick summary statistic of the dataset. The first data set `pcos` includes 541 observations and a total of 6 variables. The second data set `pcos2` includes 541 observations and a total of 45 variables. There seems to be no missing values in both datasets but there is a mixture of character and numeric column types that have to be addressed / calculated. Notice that the column names are not clear enough for readers, this will be tackled in the data preparation section.

$~$

#### Summary of the `pcos` data:
```{r, echo=FALSE}
# summary of the pcos dataset
skim(pcos)
```

$~$

#### Summary of the `pcos2` data:
```{r, echo=FALSE}
# summary of the pcos2 dataset
skim(pcos2)
```

$~$


$~$


Histograms provide valuable visual insights into the distribution patterns within a dataset. With the use of the `Data Explorer` library it allows us to effortlessly generate histograms that showcase the distribution characteristics of the entire dataset, as illustrated below.

$~$

#### `pcos` data:

Upon analyzing the variable distribution, my initial observation indicates that the variables follow a right-skewed distribution. Although the `SI..No` variable exhibits a unimodal distribution, it lacks significance and will be eliminated upon merging both datasets.

```{r, r, fig.height=4, fig.width=6, fig.align='center', warning=FALSE, echo=FALSE, message=FALSE}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 1, fill = "#7e102c"),
  title = "Fig. 1 - Histogram of `pcos` data",
   data = pcos,
         ggtheme=theme_ipsum())
```

$~$

#### `pcos2` data:

Upon visual inspection, no distinct pattern or discernible shape emerges from the histograms for the second dataset as a whole. Similar to the first dataset, `SI..No` and `Patient.File.No.` has a unimodel distribution but plays no significant role in my analysis.

```{r, fig.height = 10, fig.width = 10, fig.align='center', warning=FALSE, echo=FALSE, message=FALSE}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 1, fill = "#a86800"),
  title = "Fig. 1 - Histogram of `pcos2` data",
   data = pcos2,
         ggtheme=theme_ipsum())
```

$~$

I would like to further examine these variables but not before preparing the data set to ensure all variables are accounted for especially for `BMI`, `FSH.LH` and `Waist.Hip.Ratio`.

$~$

### Data Preparation:

My data preparation primarily involves column renaming and addressing missing values. This includes handling missing values by employing mean, median, or mode replacement strategies for columns like `Marriage Status (Yrs)` and `Fast Food (Y/N)`. Additionally, for columns such as `BMI`, `FSH.LH`, and `Waist.Hip.Ratio`, I will compute overall values. Specific columns will be converted to the metric system, and redundant or unnecessary columns will be removed. These steps aim to streamline data management before merging the two datasets.

$~$

Prior to commencing the cleaning process, I standardized the datasets to numeric format due to variations in the class of certain variables.

```{r, echo=FALSE, warning=FALSE}
pcos <- mutate_all(pcos, function(x) as.numeric(as.character(x)))
pcos2 <- mutate_all(pcos2, function(x) as.numeric(as.character(x)))
```

$~$

From the `skimr` summary it was noted that there were no missing values but there was an error in the computational values of certain columns. Below is the initial count of missing values for each data set:

`pcos` dataset:

```{r, echo=FALSE}
# missing data
colSums(is.na(pcos))
```
$~$

`pcos2` dataset:
```{r, echo=FALSE}
# missing data
colSums(is.na(pcos2))
```

$~$


```{r, echo=FALSE}
# removing first two column for `pcos` data
pcos <- dplyr::select(pcos, -c(2:6))

# renaming columns for `pcos` data
pcos <- pcos %>% 
  rename("Sl.No" = "Sl..No")

# removing columns not needed for `pcos_infertility` data
pcos2 <- dplyr::select(pcos2, -c(2, 45))

# renaming columns for `pcos_infertility` data
pcos2 <- pcos2 %>% 
  rename("Sl.No" = "Sl..No",
         "PCOS" = "PCOS..Y.N.",
         "Age_yrs" = "Age..yrs.",
         "Weight" = "Weight..Kg.",
         "Height" = "Height.Cm.",
         "BMI" = "BMI",
         "Blood_Group" = "Blood.Group", 
         "Pulse_rate_bpm" = "Pulse.rate.bpm.",
         "RR_breaths_min" = "RR..breaths.min.",
         "Hb_gdl" = "Hb.g.dl.",
         "Cycle_RI" = "Cycle.R.I.",
         "Cycle_length_days" = "Cycle.length.days.",
         "Married_yrs" = "Marraige.Status..Yrs.",
         "Pregnant" = "Pregnant.Y.N.",
         "No_of_abortions" = "No..of.aborptions",
         "IbetaHCG_mIUmL" = "I...beta.HCG.mIU.mL.",
         "IIbetaHCG_mIUmL" = "II....beta.HCG.mIU.mL.",
         "FSH_mIUmL" = "FSH.mIU.mL.",
         "LH_mIUmL" = "LH.mIU.mL.",
         "FSH_LH" = "FSH.LH",
         "Hip" = "Hip.inch.",
         "Waist" = "Waist.inch.",
         "Waist_Hip_Ratio" = "Waist.Hip.Ratio",
         "TSH_mIUmL" = "TSH..mIU.L.",
         "AMH_ngmL" = "AMH.ng.mL.",
         "PRL_ngmL" = "PRL.ng.mL.",
         "Vit_D3_ngmL" = "Vit.D3..ng.mL.",
         "PRG_ngmL" = "PRG.ng.mL.",
         "RBS_mgdl" = "RBS.mg.dl.",
         "Weight_gain" = "Weight.gain.Y.N.",
         "Hair_growth" = "hair.growth.Y.N.",
         "Skin_darkening" = "Skin.darkening..Y.N.",
         "Hair_loss" = "Hair.loss.Y.N.",
         "Pimples" = "Pimples.Y.N.",
         "Fast_food" = "Fast.food..Y.N.",
         "Reg_Exercise" = "Reg.Exercise.Y.N.",
         "BP_Systolic_mmHg" = "BP._Systolic..mmHg.",
         "BP_Diastolic_mmHg" = "BP._Diastolic..mmHg.",
         "Follicle_NoL" = "Follicle.No...L.",
         "Follicle_NoR" = "Follicle.No...R.",
         "Avg_F_sizeL_mm" = "Avg..F.size..L...mm.",
         "Avg_F_sizeR_mm" = "Avg..F.size..R...mm.",
         "Endometrium_mm" = "Endometrium..mm.")
```

$~$

After making the first part of addressing the column names, the results are seen below of the merged datasets: 

```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}
# merge data sets
pcos_data <- merge(pcos, pcos2,  by=c("Sl.No"))
pcos_data %>% 
kable(format = "html", col.names = colnames(pcos_data)) %>%
  kable_styling() %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

In the second part, I addressed missing values by initially converting 'Height' from centimeters to meters, computing 'BMI,' 'Waist-hip ratio,' and 'FSH/LH.' Following a thorough assessment of the dataset, I opted to substitute missing values in 'Marriage_Status(yrs),' 'AMH(ng/mL),' and 'Fast_food' with the median value as it didn't significantly affect the data's distribution.


```{r, echo=FALSE, warning=FALSE}
# convert Height from cm to
pcos_data$"Height" <- round((pcos_data$"Height" * 0.01),1)

# convert hip and waist from inches to cm
pcos_data$"Hip" <- round((pcos_data$"Hip" * 2.54),1)
pcos_data$"Waist" <- round((pcos_data$"Waist" * 2.54),1)

#calculate BMI
pcos_data$"BMI" <- round((pcos_data$"Weight" / pcos_data$"Height"^2), 1)

# calculate waist-hip ratio
pcos_data$"Waist_Hip_Ratio" <- round((pcos_data$"Waist" / pcos_data$"Hip"),2)

# calculate FSH/LH
pcos_data$"FSH_LH" <- round((pcos_data$"FSH_mIUmL"/pcos_data$"LH_mIUmL"),2)

# calculate Married years
pcos_data$"Married(yrs)"[is.na(pcos_data$"Married(yrs)")] <- median(pcos_data$"Married(yrs)", na.rm = T)

# calculate Fast food
pcos_data$"Fast_food"[is.na(pcos_data$"Fast_food")] <- median(pcos_data$"Fast_food", na.rm = T)

# calculate 
pcos_data$"AMH_ngmL"[is.na(pcos_data$"AMH_ngmL")] <- median(pcos_data$"AMH_ngmL", na.rm = T)

# List of variables to round
vars_to_round <- c("Hb_gdl", "Married_yrs", "IbetaHCG_mIUmL", "IbetaHCG_mIUmL", 
                   "FSH_mIUmL", "LH_mIUmL", "FSH_LH", "TSH_mIUmL", "AMH_ngmL", "PRL_ngmL", 
                   "Vit_D3_ngmL", "PRG_ngmL", "Avg_F_sizeR_mm", "Endometrium_mm")

# Rounding the variables to 1 decimal places
pcos_data <- pcos_data %>%
  mutate_at(vars(vars_to_round), ~ round(., digits = 1))


# remove 1st column
pcos_cleaned <- pcos_data[-1]
```

$~$

#### The final results are seen in `pcos_cleaned` below:
```{r,echo=FALSE}
pcos_cleaned %>% 
kable(format = "html", col.names = colnames(pcos_cleaned)) %>%
  kable_styling() %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
```

$~$

Once more, I generated a `DataExplorer` histogram to inspect the impact of the alterations on the distribution. While more variables have been included in the distribution, there isn't a consistent pattern; each variable demonstrates variations, displaying either a normal distribution, right-skewed, left-skewed, or no discernible pattern.

```{r, fig.height = 10, fig.width = 10, echo=FALSE, fig.align='center', warning=FALSE, eval=TRUE, message=FALSE}
DataExplorer::plot_histogram(
  geom_histogram_args = list(alpha = 1, fill = "dark blue"),
  title = "Fig. 3 - Histogram of `pcos_cleaned` data",
   data = pcos_cleaned,
         ggtheme=theme_ipsum())
```

$~$

The correlation plot below is measuring the degree of linear relationship within the dataset. The values in which this is measured falls between -1 and +1, with +1 being a strong positive correlation and -1 a strong negative correlation. The darker the dot the more strongly correlated (whether positive or negative). Based on the findings below, there aren't many variables that exhibit a notably strong positive or negative correlation, although some variables do fall within these categories to some extent.

Some notable correlations are as follows:

* Positive correlations between: `Weight` and `BMI`, `Married_yrs` and `Age_yrs`, and `Follicle_NoL`, `Follicle_No)` and `PCOS` to name a few.

* Negative correlations between: `Hip` and `Waist-Hip_Ratio`, `AMH_ngmL` and `Age_yrs`, and `Cycle_RI` and  `Follicle_NoR`.

```{r, fig.height=10, fig.width=10, warning=FALSE, echo=FALSE, message=FALSE}
# Selecting only the numerical variables for correlation
numerical_data <- pcos_cleaned[, sapply(pcos_cleaned, is.numeric)]

# Calculating the correlation matrix
cor_matrix <- cor(numerical_data)

# Print the correlation matrix
corrplot(cor_matrix, method = "color", type = "lower", 
         tl.col = "black", tl.cex = 0.9, title = "Fig. 4 Correlation plot of `pcos_cleaned` data",mar=c(0,0,1,0))#http://stackoverflow.com/a/14754408/54964
```

$~$

Outliers, while sometimes seen as anomalies or errors, hold significant importance in data analysis for various reasons. They can substantially influence statistical measures, potentially skewing interpretations of central tendency (mean, median, mode) and dispersion (variance, standard deviation). Their presence might distort the true characteristics of the data distribution. Outliers can adversely affect the performance of predictive models. Many machine learning algorithms are sensitive to outliers, leading to biased predictions or reduced model accuracy. Identifying and handling outliers appropriately is crucial for robust model building. Recognizing outliers aids in improving data quality by either correcting errors or understanding exceptional cases in the dataset. If handled properly, outliers can enhance our understanding of the dataset, improve model performance, and contribute to more accurate and meaningful insights. Detecting and managing outliers is essential for maintaining the integrity and reliability of data-driven analyses. With the help of boxplots we can identify outliers  across a complete dataset. 

According to the visualizations provided, a handful of outliers are evident. Considering the dataset's relatively small size, I've chosen to retain these outliers to ensure the inclusion of every individual data point as it represents natural variations in the population.

```{r, fig.height=18, fig.width=10, warning=FALSE, echo=FALSE, message=FALSE, fig.align='center'}

# boxplot of the variables with the outlier parameters
pcos_df2 <- pcos_cleaned %>%
                gather(variable, values, 1:dim(pcos_cleaned)[2])
pcos_df2 %>%
  ggplot() +
  geom_boxplot(aes(x = variable, y = values)) +
  facet_wrap(~variable, ncol = 4, scales = "free") +
  ggtitle("Fig. 5 - Boxplot outliers for `pcos_cleaned` data") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=12)
    )
```

$~$

#### Further visualizations:

Before diving into the model-building phase of this project, let's further visualize the variables with a series of scatter plots and bar charts.


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=12, fig.width=12, fig.align='center'}
p1 <- pcos_cleaned %>% 
  ggplot(aes(x=`Age_yrs`, y=`Weight`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Age(yrs) with Weight(kg)",
        x ="Age(yrs)", y = "Weight(kg)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p2 <- pcos_cleaned %>% 
  ggplot(aes(x=`Hip`, y=`Waist`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Relationship between Hip(cm) and Waist(cm)",
        x ="Hip (cm)", y = "Waist (cm)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p3 <- pcos_cleaned %>% 
  ggplot(aes(x=`Waist_Hip_Ratio`, y=`RBS_mgdl`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Relationship between Waist-Hip ration and Glucose",
        x ="Waist-Hip Ratio", y = "RBS (mg/dl)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p4 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`Cycle_length_days`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Length of cycle based on Age",
        x ="Age(yrs)", y = "Cycle Length(days)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p5 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`Cycle_RI`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Type of cycle based on Age",
        x ="Age(yrs)", y = "Cycle(R/I)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p6 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=BMI, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="BMI based on Age",
        x ="Age(yrs)", y = "BMI") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p7 <- pcos_cleaned %>% 
ggplot(aes(x=`Follicle_NoR`, y=`Follicle_NoL`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Number of Follicles",
        x ="Follicles Right", y = "Follicles Left") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p8 <- pcos_cleaned %>% 
ggplot(aes(x=`Avg_F_sizeR_mm`, y=`Avg_F_sizeL_mm`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Follicle size's",
        x ="Follicle size right", y = "Follicle size left") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p9 <- pcos_cleaned %>% 
ggplot(aes(x=`Endometrium_mm`, y=`Cycle_length_days`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Endometrium and length of cycle(days)",
        x ="Endometrium(mm)", y = "Cycle Length(days)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

ggp_all <- (p1 + p2 + p3) / (p4 + p5 + p6) / (p7 + p8 + p9) +    # Create grid of plots with title
  plot_annotation(title = "Fig. 6 - Scatterplot of variables with `PCOS (Y/N)` as factor") & 
  theme(plot.title = element_text(hjust = 0.5))
ggp_all  

# displaying the histograms
# par(mfrow = c(3, 3))
# grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9)

```




```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=12, fig.width=12, fig.align='center'}

p10 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`PCOS`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="PCOS", x ="PCOS(Yes or No)", y = "count") +
   theme_ipsum() +
      theme(
        plot.title = element_text(size=10)
      )

p11 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Pregnant), fill = as.factor(Pregnant))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Pregnant", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Pregnant", x ="Pregnant(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p12 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Weight_gain), fill = as.factor(Weight_gain))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Weight Gain", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Weight Gain", x ="Weight Gain (Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p13 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Hair_growth), fill = as.factor(Hair_growth))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Hair Growth", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Hair Growth", x ="Hair Growth(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p14 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Skin_darkening), fill = as.factor(Skin_darkening))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Skin Darkening", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Skin Darkening", x ="Skin Darkening (Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p15 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Hair_loss), fill = as.factor(Hair_loss))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Hair Loss", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Hair Loss", x ="Hair Loss(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p16 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Pimples), fill = as.factor(Pimples))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Pimples", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Pimples", x ="Pimples(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p17 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Fast_food), fill = as.factor(Fast_food))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Fast Food", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Fast Food", x ="Fast Food(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p18 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Reg_Exercise), fill = as.factor(Reg_Exercise))) +
    geom_bar(position = "dodge") + 
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    scale_fill_discrete(name = "Regular Exercise", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    labs(title="Regularly Exercise", x ="Regular Exercise(Yes or No)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

ggp_all2 <- (p10 + p11 + p12) / (p13 + p14 + p15) / (p16 + p17 + p18) +    # Create grid of plots with title
  plot_annotation(title = "Fig. 7 - Bar charts of Yes or No variables") & 
  theme(plot.title = element_text(hjust = 0.5))
ggp_all2 

# displaying the histograms
# par(mfrow = c(3, 3))
# grid.arrange(p10,p11,p12,p13,p14,p15,p16,p17,p18)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=12, fig.width=12, fig.align='center'}
p19 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Blood_Group), fill = as.factor(Blood_Group))) +
    geom_bar(show.legend = FALSE) +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black") +
  labs(title="Blood Groups",
        x ="Blood Groups", y = "count") +
  scale_x_discrete(labels=c('A+', 'A-', 'B+', 'B-', 'O+', 'O-', 'AB+', 'AB-')) +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p20 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Age_yrs`))) +
    geom_histogram(stat="count", show.legend = FALSE, , fill = "#69b3a2", color = "#e9ecef", alpha=0.9) +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
  labs(title="Age range", x ="Age(yrs)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p21 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Weight`))) +
    geom_histogram(stat="count", show.legend = FALSE, , fill = "#69b3a2", color = "#e9ecef", alpha=0.9) +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    labs(title="Weight range", x ="Weight(kg)", y = "count") +
    theme_ipsum() +
      theme(
        plot.title = element_text(size=10), axis.text.x = element_text(angle = 90, vjust = 1, hjust=2, size=5)
    )

p22 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Height`))) +
    geom_histogram(stat="count", show.legend = FALSE, , fill = "#69b3a2", color = "#e9ecef", alpha=0.9) +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    labs(title="Height range", x ="Height (m)", y = "count") +
    theme_ipsum() +
      theme(
        plot.title = element_text(size=10), axis.text.x = element_text(angle = 90, vjust = 1, hjust=2, size=10)
    )

p23 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Married_yrs`))) +
    geom_histogram(stat="count", show.legend = FALSE, fill = "#69b3a2", color = "#e9ecef", alpha=0.9) +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    labs(title="Years married range", x ="Married (yrs)", y = "count") +
    theme_ipsum() +
      theme(
        plot.title = element_text(size=10)
    )

p24 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`PRG_ngmL`))) +
  geom_histogram(stat="count", show.legend = FALSE, , fill = "#69b3a2", color = "#e9ecef", alpha=0.9) +
  labs(title="PRG range", x ="PRG(ng/mL)", y = "count") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 90, vjust = 1, hjust=2, size=5)
    )

ggp_all3 <- (p19 + p20 + p21) / (p22 + p23 + p24) +    # Create grid of plots with title
  plot_annotation(title = "Fig. 8 - Scatterplot of Yes or No variables") & 
  theme(plot.title = element_text(hjust = 0.5))
ggp_all3 

# displaying the histograms
# par(mfrow = c(2, 3))
# grid.arrange(p19,p20,p21,p22,p23,p24)
```



```{r, echo=FALSE, warning=FALSE, fig.height=10, fig.width=12, message=FALSE}
p25 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`Cycle_length_days`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Length of cycle based on Age",
        x ="Age(yrs)", y = "Cycle Length(days)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p26 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`Cycle_RI`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Type of cycle based on Age",
        x ="Age(yrs)", y = "Cycle(R/I)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p27 <- pcos_cleaned %>% 
ggplot(aes(x=`Age_yrs`, y=`BMI`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="BMI based on Age",
        x ="Age(yrs)", y = "BMI") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p28 <- pcos_cleaned %>% 
ggplot(aes(x=`Follicle_NoR`, y=`Follicle_NoL`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Number of Follicles",
        x ="Follicles Right", y = "Follicles Left") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p29 <- pcos_cleaned %>% 
ggplot(aes(x=`Avg_F_sizeR_mm`, y=`Avg_F_sizeL_mm`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Follicle size's",
        x ="Follicle size right", y = "Follicle size left") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p30 <- pcos_cleaned %>% 
ggplot(aes(x=`Endometrium_mm`, y=`Cycle_length_days`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Endometrium and length of cycle(days)",
        x ="Endometrium(mm)", y = "Cycle Length(days)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

ggp_all4 <- (p25 + p26 + p27) / (p28 + p29 + p30) +    # Create grid of plots with title
  plot_annotation(title = "Fig. 9 - Scatterplot of variables with `PCOS (Y/N)` as factor") & 
  theme(plot.title = element_text(hjust = 0.5))
ggp_all4 


# displaying the histograms
# par(mfrow = c(3, 3))
# grid.arrange(p25,p26,p27,p28,p29,p30)
```

$~$


```{r, echo=FALSE, warning=FALSE, fig.height=10, fig.width=12, message=FALSE}
p31 <- pcos_cleaned %>% 
ggplot(aes(x=`IbetaHCG_mIUmL`, y=`IIbetaHCG_mIUmL`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Pregnancy hormone levels",
        x ="I Beta-HCG(mIu/mL)", y = "II Beta-HCG(mIu/mL)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p32 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Vit_D3_ngmL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Vitamin D3 levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

p33 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`FSH_LH`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("FSH/LH levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

p34 <- pcos_cleaned %>% 
ggplot(aes(x=`BP_Diastolic_mmHg`, y=`BP_Systolic_mmHg`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Blood Pressure levels",
        x ="BP_Diastolic(mmHg)", y = "BP_Systolic(mmHg)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p35 <- pcos_cleaned %>% 
ggplot(aes(x=`RR_breaths_min`, y=`Pulse_rate_bpm`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Respiration rate vs Pulse rate(bpm)",
        x ="RR(breaths/min)", y = "Pulse rate(bpm)") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p36 <- pcos_cleaned %>% 
ggplot(aes(x=`AMH_ngmL`, y=`FSH_LH`, color=as.factor(`PCOS`))) + 
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
    scale_colour_discrete("PCOS", labels = c("No", 
    "Yes")) +
  labs(title="Ovarian reserve against follicle growth",
        x ="Ovarian reserve", y = "Growth of ovarian follicles") +
  theme_ipsum() +
    theme(
      plot.title = element_text(size=10)
    )

p37 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`TSH_mIUL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("TSH (mIU/L) levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

p38 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`Hb_gdl`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Hb(g/dl) levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

p39 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`PRL_ngmL`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("PRL (ng/mL) levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

p40 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(`RBS_mgdl`), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("RBS (mg/dl) levels with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
    )

ggp_all5 <- (p10 + p11 + p12) / (p13 + p14 + p15) / (p16 + p17 + p18) +    # Create grid of plots with title
  plot_annotation(title = "Fig. 10 - Bar charts of Yes or No variables") & 
  theme(plot.title = element_text(hjust = 0.5))
ggp_all5 

# displaying the histograms
# par(mfrow = c(4, 3))
# grid.arrange(p31,p32,p33,p34,p35,p36,p37,p38,p39,p40)
```

$~$

```{r, echo=FALSE, warning=FALSE, fig.height=12, fig.width=14, message=FALSE}
p41 <- pcos_cleaned %>% 
  ggplot(aes(x = Blood_Group, fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Blood Group with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p42 <- pcos_cleaned %>% 
  ggplot(aes(x = as.factor(Pregnant), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Pregnant with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No', 'Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p43 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Weight_gain), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Weight gain with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p44 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Hair_growth), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Hair growth with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p45 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Skin_darkening), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Skin darkening with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p46 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Hair_loss), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Hair loss with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p47 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Pimples), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Pimples with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p48 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Fast_food), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Fast food consumption with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p49 <- pcos_cleaned %>%
  ggplot(aes(x = as.factor(Reg_Exercise), fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Regularly exercises with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    scale_x_discrete(labels=c('No','Yes')) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p50 <- pcos_cleaned %>% 
  ggplot(aes(x = `Cycle_length_days`, fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    #geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
    ggtitle("Cycle length in days with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

p51 <- pcos_cleaned %>% 
  ggplot(aes(x = No_of_abortions, fill = as.factor(`PCOS`))) +
    geom_bar(position = "dodge") +
    ggtitle("Number of abortions with PCOS") +
    scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
    theme_ipsum() +
    theme(
      plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
    )

ggp_all6 <- (p10 + p11 + p12) / (p13 + p14 + p15) / (p16 + p17 + p18) +    # Create grid of plots with title
  plot_annotation(title = "Fig. 11 - Bar charts of Yes or No variables` as factor") & 
  theme(plot.title = element_text(hjust = 0.5))
ggp_all6

# displaying the histograms
# par(mfrow = c(4, 3))
# grid.arrange(p41,p42,p43,p44,p45,p46,p47,p48,p49,p50,p51)
```

$~$


### Model Building:

As previously mentioned, I've decided for the prediction anaylsis to do Decision Tree (rpart), Random Forest (randomForest), Gradient Boosting Machines (xgboost or gbm), Support Vector Machines (e1071), Neural Networks (neuralnet), and K-Nearest Neighbors (kknn or class). 

Decision Tree (rpart): Decision trees recursively split the dataset based on features to make decisions. They represent a tree-like structure where nodes correspond to features, branches represent decisions, and leaves denote outcomes. R's rpart package builds decision trees using the Recursive Partitioning and Regression Trees algorithm.

Random Forest (randomForest): Random Forest is an ensemble learning method that constructs multiple decision trees and merges their predictions to improve accuracy and reduce overfitting. The randomForest package in R implements this ensemble method, creating a collection of decision trees and aggregating their predictions.

Gradient Boosting Machines (xgboost or gbm): Gradient Boosting Machines (GBMs) create an ensemble of weak learners (typically decision trees) sequentially, where each new tree corrects the errors made by the previous one. R provides packages like xgboost and gbm to implement gradient boosting, allowing users to build efficient and optimized boosting models.

Support Vector Machines (e1071): Support Vector Machines are supervised learning models used for classification or regression tasks. They find a hyperplane that best separates data into different classes while maximizing the margin. The e1071 package in R includes SVM implementations for classification and regression tasks.

Neural Networks (neuralnet): Neural Networks are a set of algorithms designed to recognize patterns. They consist of interconnected nodes (neurons) organized in layers and can model complex relationships in data. The neuralnet package in R allows users to create and train neural networks for various tasks like regression and classification.

K-Nearest Neighbors (kknn or class): K-Nearest Neighbors (KNN) is a simple, instance-based learning algorithm. It classifies new data points based on majority voting from the k-nearest data points in the training set. R provides packages like kknn or class to implement KNN algorithms for both classification and regression tasks.

$~$

We start by splitting the dataset into training and validation sets for machine learning models. The training set is used to train the model, while the validation set helps evaluate its performance. The 75:25 ratio used strikes a balance between having enough data to train the model effectively and having a substantial validation set for robust evaluation.

```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(29)

# Cross Validation Set-up
inTrain <- createDataPartition(pcos_cleaned$`PCOS`, p=.75, list = F)
train <- pcos_cleaned[inTrain,]
valid <- pcos_cleaned[-inTrain,]
```

$~$

#### Decision Tree (rpart):

The initial decision tree constructed is based on `PCOS` against the entire dataset. Employing a 75:25 cross-validation approach to divided the data I created the decision tree. The output is below:

```{r, echo=FALSE, fig.height=10, fig.width=10, fig.align='center'}
# create the decision tree
rpart_model <- rpart(`PCOS` ~ ., method = "class", data = train)

# display the decision tree
prp(rpart_model, main = "Fig. 12 - Decision Tree with entire dataset", extra=1, faclen=0,  nn=T, box.palette="Blues")
```

$~$

Then we test the model using the validation dataset. The results are seen in the confusion matrix and statistics output.

The confusion matrix shows the following layout:

* True Negative (TN) = 88: Actual class 0 (negative) correctly predicted as 0.
* False Positive (FP) = 13: Actual class 0 incorrectly predicted as 1 (positive).
* False Negative (FN) = 6: Actual class 1 incorrectly predicted as 0.
* True Positive (TP) = 28: Actual class 1 correctly predicted as 1.

Accuracy and Confidence Interval (CI):

Accuracy: 85.93% - The proportion of correctly classified instances out of the total instances.
95% Confidence Interval: The range within which the true accuracy is likely to lie.

Other Statistics:

* No Information Rate (NIR): The accuracy rate if the model simply predicted the majority class for all instances.

Interpretation:

* The model has a relatively high accuracy (85.93%), suggesting it correctly predicts classes in the dataset.
* Sensitivity (True Positive Rate) is high (93.62%), indicating that the model is good at identifying actual positive instances.
* Specificity (True Negative Rate) is moderate (68.29%), suggesting a decent ability to identify actual negative instances.
* The positive predictive value (Precision) is 87.13%, indicating the proportion of correctly predicted positive instances out of all positive predictions.

There might be a class imbalance issue, considering the differences in sensitivity and specificity. Overall, while the model shows relatively good performance, further analysis might be required to understand its behavior, especially if sensitivity or specificity is more critical for the specific application or domain.

```{r,echo=FALSE}
# creating our prediction
rpart_result <- predict(rpart_model, newdata = valid[, !colnames(valid) %in% "PCOS"], type = 'class')

# confusion matrix
confusionMatrix(rpart_result, as.factor(valid$`PCOS`))
```

$~$

Upon examining the individual contributions of each variable. It's evident that `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, and `Weight_gain`, hold significant influence within the dataset. I expected a wider range of influential bloodwork-related tests beyond those presented in this outcome.

```{r, echo=FALSE}
# contribution of variables
varImp(rpart_model) %>% kable()
```
$~$

The model's accuracy stands at 85.93%, which is commendable. However, considering the variable contributions, I'm planning to build a second decision tree by removing the variables with lower contributions.

```{r, echo=FALSE}
# Extract accuracy from the confusion matrix
accuracy_rpart <- confusionMatrix(rpart_result, as.factor(valid$`PCOS`))$overall["Accuracy"]
kable(accuracy_rpart, align = "l")
```
$~$

##### Second Decision Tree:

The second decision tree is based off the variables with the highest contribution: `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, `Weight_gain` and the target variable `PCOS`. I also created a second cross validation data set that include the 6 variables chosen. The results are below:

```{r, echo=FALSE, fig.height=10, fig.width=10, fig.align='center'}
# creating the second dataset from the original
pcos_cleaned2 <- pcos_cleaned %>%
  select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)

# create some random number for reproduction
set.seed(28)

# Cross Validation Set-up
inTrain2 <- createDataPartition(pcos_cleaned2$`PCOS`, p=.75, list = F)
train2 <- pcos_cleaned2[inTrain2,]
valid2 <- pcos_cleaned2[-inTrain2,]

# create the decision tree
rpart_model2 <- rpart(`PCOS` ~ ., method = "class", data = train2)

# display the decision tree
prp(rpart_model2, main = "Fig. 13 - Second Decision Tree with 6 variables", extra=1, faclen=0,  nn=T, box.palette="Blues")
```

$~$

Same as before, we create the confusion matrix and statistics for the second decision tree using the validation data:

Interpretation:

* The model displays high accuracy (91.85%) in correctly predicting classes.
* Sensitivity (True Positive Rate) is very high (98.94%), indicating an excellent ability to identify actual positive instances.
* Specificity (True Negative Rate) is moderate (75.61%), suggesting a decent ability to identify actual negative instances.
* Positive Predictive Value (Precision) is 90.29%, meaning that around 90.29% of the predicted positive instances are actually positive.

The model seems to perform well in this dataset, with high sensitivity and specificity, indicating robustness in classifying both positive and negative instances effectively.
```{r, echo=FALSE}
# creating our prediction
rpart_result2 <- predict(rpart_model2, newdata = valid2[, !colnames(valid2) %in% "PCOS"], type = 'class')

# creating the third confusion matrix
confusionMatrix(rpart_result2, as.factor(valid2$`PCOS`))
```

$~$

Let's examine the impact of each variable within the context of the second dataset. There's a noticeable shift in the overall contribution levels among the variables.

```{r, echo=FALSE}
# contribution of variables
varImp(rpart_model2) %>% kable()
```
$~$

Ultimately, we observe that the accuracy has increased to 91.85% compared to the initial decision tree.

```{r, echo=FALSE}
# Extract accuracy from the confusion matrix
accuracy_rpart2 <- confusionMatrix(rpart_result2, as.factor(valid2$`PCOS`))$overall["Accuracy"]
kable(accuracy_rpart2, align = "l")
```

$~$

#### Random Forest (randomForest):

The second model we create a random forest model for the entire dataset. Following similar steps as the decision tree, the model was trained using the entire dataset. This produced an error that wouldn't display a confusion matrix. Despite my efforts, the first model wasn't achieved. I was however, able to produce a plot of the variable contribution seen below:

```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(30)

# Cross Validation Set-up
rf_inTrain <- createDataPartition(pcos_cleaned$`PCOS`, p=.75, list = F)
rf_train <- pcos_cleaned[rf_inTrain,]
rf_valid <- pcos_cleaned[-rf_inTrain,]
```


```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(rf_train$PCOS)
levels(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in svm_train
rf_train$PCOS <- factor(rf_train$PCOS)

# Convert PCOS to factor in svm_valid
rf_valid$PCOS <- factor(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(rf_train$PCOS)
levels(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in svm_train.
rf_valid$PCOS <- factor(rf_valid$PCOS, levels = levels(rf_train$PCOS))
levels(rf_valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Check the length of svm_result and svm_valid$PCOS
length_rf_result <- length(rf_result)
length_rf_valid <- length(rf_valid$PCOS)

# Print the lengths for comparison
print(length_rf_result)
print(length_rf_valid)
```

```{r, echo=FALSE, error = TRUE}
#create some random number for reproduction
set.seed(39)

# create random forest model using the training data
rf_model <- randomForest(PCOS~., rf_train)
rf_model

# prediction
rf_result <- predict(rf_model, newdata = valid[, !colnames(valid) %in% "PCOS"])

# Create a confusion matrix
confusionMatrix(data = rf_result, reference = rf_valid$PCOS)
```

```{r, echo=FALSE}
# plot for rf_model
varImpPlot(rf_model)
```

```{r, echo=FALSE, error=TRUE}
# Extract accuracy from the confusion matrix for the rf_model
accuracy_rf <- confusionMatrix(rf_result, valid$PCOS)$overall["Accuracy"]
accuracy_rf
```

$~$

#### Second Random Forest Model:

The second random forest model was created using the 6 variables `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, `Weight_gain` and the target variable `PCOS`. After training this model, the following confusion matrix was the result.

```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(78)

# Cross Validation Set-up
rf_inTrain2 <- createDataPartition(pcos_cleaned2$`PCOS`, p=.75, list = F)
rf_train2 <- pcos_cleaned2[rf_inTrain2,]
rf_valid2 <- pcos_cleaned2[-rf_inTrain2,]
```

```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(rf_train2$PCOS)
levels(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in svm_train
rf_train2$PCOS <- factor(rf_train2$PCOS)

# Convert PCOS to factor in svm_valid
rf_valid2$PCOS <- factor(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(rf_train2$PCOS)
levels(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in svm_train.
rf_valid2$PCOS <- factor(rf_valid2$PCOS, levels = levels(rf_train2$PCOS))
levels(rf_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Check the length of svm_result and svm_valid$PCOS
length_rf_result2 <- length(rf_result2)
length_rf_valid2 <- length(rf_valid2$PCOS)

# Print the lengths for comparison
print(length_rf_result2)
print(length_rf_valid2)
```


```{r, echo=FALSE}
# create some random number for reproduction
set.seed(7)

# create the second random forest model using the training data from the third decision tree
rf_model2 <- randomForest(PCOS ~ Follicle_NoR + Follicle_NoL + Weight_gain + Skin_darkening + Hair_growth, data = rf_train2)
rf_model2

# creating the prediction for the third decision tree
rf_result2 <- predict(rf_model2, newdata = rf_valid2[, !colnames(rf_valid2) %in% "PCOS"])

# Convert PCOS column to factor in rf_train2 and rf_valid2
rf_train2$PCOS <- factor(rf_train2$PCOS)
rf_valid2$PCOS <- factor(rf_valid2$PCOS)

# # Check unique levels in rf_result2 and rf_valid2$PCOS
# unique_levels_result <- unique(rf_result2)
# unique_levels_valid <- unique(rf_valid2$PCOS)
# 
# # Check if the levels match
# identical(unique_levels_result, unique_levels_valid)
# 
# # If levels do not match, manually set levels in rf_result2 to match those in rf_valid2$PCOS
# levels(rf_result2) <- levels(rf_valid2$PCOS)
# 
# Convert rf_result2 to factor and align levels with rf_valid2$PCOS
rf_result2_factor <- factor(rf_result2, levels = levels(rf_valid2$PCOS))

# Create a confusion matrix
confusionMatrix(data = rf_result2_factor, reference = rf_valid2$PCOS)
```

$~$

From the random forest model we created, we can create a variable importance plot which shows each variable and how important it is in classifying the data. From the plot below we note that `Follicle_NoR` and `Follicle_NoL` are among the top variables that play a significant role in the classification of having or not having PCOS .

```{r, echo=FALSE}
# plot for the second rf_model
varImpPlot(rf_model2)
```

$~$

Numerically, we can see the same result below:
```{r, echo=FALSE}
# table for rf_model2 variable contribution
varImp(rf_model2) %>% kable()
```

$~$

Lastly, I checked against the validation data the accuracy of the second model with the results of 90.37% accuracy.

```{r, echo=FALSE}
# Extract accuracy from the confusion matrix for the rf_model2
accuracy_rf2 <- confusionMatrix(data = rf_result2_factor, reference = rf_valid2$PCOS)$overall["Accuracy"]
accuracy_rf2
```

$~$

#### Gradient Boosting Machines (xgboost or gbm):

The first gbm model is created using the entire dataset with the target variable `PCOS`. 

In a GBM model, the "rel.inf" typically denotes the relative importance of each feature in predicting the target variable. `Follicle_NoR` is identified as the most crucial variable in the model, followed by others that contribute to a lesser extent.

```{r, echo=FALSE}
# Set seed for reproducibility
set.seed(67)

# Train the GBM model
gbm_model <- gbm(`PCOS` ~ ., data = train, distribution = "bernoulli", n.trees = 100, interaction.depth = 4, shrinkage = 0.01, bag.fraction = 0.5)

# Print the summary of the trained model
summary(gbm_model)

# Predict on the validation dataset (assuming 'valid' contains your validation dataset)
gbm_pred <- predict(gbm_model, newdata = valid, type = "response")
```

$~$

We create the confusion matrix and statistics for the first gbm. The model exhibits strong overall accuracy, substantial agreement (Kappa), and good performance in correctly identifying positive and negative classes, as indicated by sensitivity, specificity, and predictive values.

```{r, echo=FALSE}
# Calculate predicted classes (0 or 1) based on the predicted probabilities
predicted_classes <- ifelse(gbm_pred > 0.5, 1, 0)

# Create confusion matrix
confusionMatrix(data = factor(predicted_classes), reference = factor(valid$`PCOS`))
```

$~$

The accuracy of this model is 88.89% which is good but a second model will be created with the selected variables.
```{r, echo=FALSE}
# Calculate accuracy
gbm_accuracy <- sum(predicted_classes == valid$`PCOS`) / length(valid$`PCOS`)
cat("Accuracy:", gbm_accuracy)
```


$~$

##### Second GBM:

The second GBM is based off the variables: `Follicle_NoL`, `Follicle_NoR`, `Hair_growth`, `Skin_darkening`, `Weight_gain` and the target variable `PCOS`. The results are below:

Similar to before: `Follicle_NoR` is identified as the most crucial variable in the model, with the increased rel.inf of 65.135909 followed by others that contribute to a lesser extent.

```{r, echo=FALSE}
# creating the second dataset from the original
pcos_cleaned3 <- pcos_cleaned %>%
  select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)

# Set seed for reproducibility
set.seed(68)

# Cross Validation Set-up
inTrain3 <- createDataPartition(pcos_cleaned3$`PCOS`, p=.75, list = F)
train3 <- pcos_cleaned3[inTrain3,]
valid3 <- pcos_cleaned3[-inTrain3,]

# Train the GBM model
gbm_model2 <- gbm(`PCOS` ~ ., data = train3, distribution = "bernoulli", n.trees = 100, interaction.depth = 4, shrinkage = 0.01, bag.fraction = 0.5)

# Print the summary of the trained model
summary(gbm_model2)

# Predict on the validation dataset (assuming 'valid' contains your validation dataset)
gbm_pred2 <- predict(gbm_model2, newdata = valid3, type = "response")
```

$~$

The model also exhibits a reasonably good accuracy and agreement between actual and predicted classes. It shows higher sensitivity than the first model but relatively lower specificity. The model's overall performance is moderately robust but might have some limitations in correctly identifying negative instances (class 0).

```{r, echo=FALSE}
# Calculate predicted classes (0 or 1) based on the predicted probabilities
predicted_classes2 <- ifelse(gbm_pred2 > 0.5, 1, 0)

# Create confusion matrix
confusionMatrix(data = factor(predicted_classes2), reference = factor(valid3$`PCOS`))
```

$~$

The accuracy observed in this subsequent GBM model is slightly reduced compared to the initial one, achieving an 88.15% accuracy rate.
```{r, echo=FALSE}
# Calculate accuracy
gbm_accuracy2 <- sum(predicted_classes2 == valid3$`PCOS`) / length(valid3$`PCOS`)
cat("Accuracy:", gbm_accuracy2)
```


$~$

#### Support Vector Machines (e1071):



```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(train$PCOS)
levels(valid$PCOS)
```
```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in svm_train
train$PCOS <- factor(train$PCOS)

# Convert PCOS to factor in svm_valid
valid$PCOS <- factor(valid$PCOS)
```


```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(train$PCOS)
levels(valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# checking the structure of both valid and train datasets
str(valid)
str(train)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in svm_train.
valid$PCOS <- factor(valid$PCOS, levels = levels(train$PCOS))
levels(valid$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Check the length of svm_result and svm_valid$PCOS
length_svm_result <- length(svm_result)
length_svm_valid <- length(svm_valid$PCOS)

# Print the lengths for comparison
print(length_svm_result)
print(length_svm_valid)
```


```{r, echo=FALSE, error = TRUE }
#create some random numbers for reproduction
set.seed(31)

# SVM
svm_model <- svm(PCOS ~ ., train)

# create prediction
svm_result <- predict(svm_model, newdata = valid)


# confusion matrix for svm
confusionMatrix(svm_result, valid$PCOS)
```

```{r, echo=FALSE, error=TRUE}
summary(svm_result)
```

```{r, echo=FALSE, error=TRUE}
#Extract accuracy from the confusion matrix
accuracy_svm <- confusionMatrix(svm_result, as.factor(valid$`PCOS`))$overall["Accuracy"]
accuracy_svm
```

$~$

##### Second SVM

```{r}
# create some random numbers for reproduction
set.seed(8)

# Cross Validation Set-up
svm_inTrain2 <- createDataPartition(pcos_cleaned2$PCOS, p=.75, list = FALSE)
svm_train2 <- pcos_cleaned2[svm_inTrain2,]
svm_valid2 <- pcos_cleaned2[-svm_inTrain2,]
```

```{r, echo=FALSE, include=FALSE}
# check the levels of PCOS using levels() 
levels(svm_train2$PCOS)
levels(svm_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Convert PCOS to factor in svm_train
svm_train2$PCOS <- factor(svm_train2$PCOS)

# Convert PCOS to factor in svm_valid
svm_valid2$PCOS <- factor(svm_valid2$PCOS)

```

```{r, echo=FALSE, include=FALSE}
# rechecking levels again to ensure no NULL values
levels(svm_train2$PCOS)
levels(svm_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# explicitly set the levels to match the levels in svm_train.
valid$PCOS <- factor(svm_valid2$PCOS, levels = levels(svm_train2$PCOS))
levels(svm_valid2$PCOS)
```

```{r, echo=FALSE, include=FALSE}
# Check the length of svm_result and svm_valid$PCOS
length_svm_result <- length(svm_result)
length_svm_valid2 <- length(svm_valid2$PCOS)

# Print the lengths for comparison
print(length_svm_result)
print(length_svm_valid2)
```

$~$

The confusion matrix for the second SVM:
```{r, echo=FALSE, error = TRUE}
# SVM
svm_model2 <- svm(PCOS ~ Follicle_NoR + Follicle_NoL + Weight_gain + Skin_darkening + Hair_growth, svm_train2)

# create prediction
svm_result2 <- predict(svm_model2, newdata = svm_valid2)

# confusion matrix for svm
confusionMatrix(svm_result2, svm_valid2$PCOS)
```


```{r, echo=FALSE, error=TRUE}
summary(svm_result2)
```

```{r, echo=FALSE, error=TRUE}
#Extract accuracy from the confusion matrix
accuracy_svm2 <- confusionMatrix(svm_result2, svm_valid2$`PCOS`)$overall["Accuracy"]
accuracy_svm2
```


$~$

#### Neural Networks (neuralnet):

Utilizing the neuralnet package in R I created the neural network models. The neural network is constructed to predict the target variable `PCOS` using all available predictors. The model is then trained using the training dataset using hidden argument that help specify the architecture of the neural network. These included setting up two hidden layers with 12 and 8 neurons, respectively. The stepmax parameter determines the maximum number of iterations allowed during training, here set to 20000 to ensure the model has enough iterations to learn from the data and optimize its parameters.

For context, neural network plots are composed of:

* The left-most nodes (i.e. input nodes) are the raw data variables used in the model.

* The arrows in black (and associated numbers) are the weights which you can think of as how much that variable contributes to the next node. The blue lines are the bias weights which are an additional parameter introduced to each neuron and serves as an offset or intercept term, contributing to the ability of the network to fit more complex patterns in the data.

* The middle nodes (i.e. anything between the input and output nodes) are your hidden nodes. This is where the image analogy helps. Each of these nodes constitute a component that the network is learning to recognize. 

* The far-right (output node(s)) node is the final output of the neural network.


```{r,echo=FALSE}
# create some random numbers for reproduction
set.seed(67)

# Cross Validation Set-up
nn_inTrain <- createDataPartition(pcos_cleaned$PCOS, p=.75, list = F)
nn_train <- pcos_cleaned[nn_inTrain,]
nn_valid <- pcos_cleaned[-nn_inTrain,]
```


```{r, echo=FALSE, error=TRUE}
# set a seed for reproducibility purposes
set.seed(19)

# create the model
nn_model <- neuralnet(`PCOS`~.,
                      data = nn_train,
                      hidden = c(12, 8),  # Specify the number of hidden layers and neurons
                      linear.output = FALSE,
                      stepmax = 20000  # Increase the maximum number of iterations
)
```

Although not displayed when knitting, the plot produced is seen below:

![Fig. 15 - Neural Network 1](/Users/letisalba/Desktop/Data-698/Final-Presentation-and-Paper/Fig.15-Neural_Network_1.png)

```{r, echo=FALSE, fig.width=18, fig.height=12, fig.align='center', include=FALSE, error=TRUE}
# create the plot based on the model above
#plot(nn_model, rep = "best", main="")
#grid::grid.text("Fig. 15 - Neural Network", x = 0.5, y = 0.1)
```

```{r, echo=FALSE, error=TRUE}
# make predictions on the test data using a previously trained model
pred <- predict(nn_model, valid)

# create a vector of labels for the two possible `PCOS(Y/N)` status in the dataset.
labels <- c("0", "1")

# creates a data frame with the column index of the maximum value in each row of the "pred" variable
prediction_label <- data.frame(max.col(pred)) %>%
# use the mutate function to add a new column to the data frame called "pred"
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
# convert the data frame to a vector.
unlist()

# print the table
table(valid$`PCOS`, prediction_label)
```


$~$

The accuracy of the model above was of 30.37%. Hoping this could improve in the second model.

```{r, echo=FALSE, error=TRUE}
#checking the accuracy
check <- as.numeric(valid$`PCOS`) == max.col(pred)
nn_accuracy <-  (sum(check)/nrow(valid))
nn_accuracy
```

$~$

##### Second Neural Network:

For the second neural network model I am also using the `neuralnet` package. Similar to the first model, I created the model by specifying the variables from `pcos_cleaned2` which included `Follicle_No(L)`, `Follicle_No(R)`, `Hair_growth`, `Skin_darkening`, `Weight_gain`, and the target variable `PCOS` and setting the parameters to 1 hidden layer containing 2 neurons and an output layer with 1 neuron.

```{r, echo=FALSE}
# set a seed for reproducibility purposes
set.seed(13)

# create the model
nn_model2 <-  neuralnet(`PCOS`~Follicle_NoL + Follicle_NoR + Hair_growth + Skin_darkening + Weight_gain,
                        data=train2,
                        hidden=c(2,1),
                        linear.output = FALSE,
                        stepmax = 10000  # Increase the maximum number of iterations
)
```

$~$

The plot below is the output of the model created:

In the context of a `neuralnet` plot displaying an error of 13.733636 and steps of 2518, this information typically pertains to the training progress of a neural network model and provides insights into the model's learning behavior.

* An error value of 13.733636 suggests the current level of model error or loss at the 2518th step/iteration during training.

* Each step involves adjusting the model's weights and biases to reduce the error, moving closer towards an optimal solution. Higher step counts usually mean more training iterations were required for the network to converge.

```{r, echo=FALSE}
# create the plot based on the model above
plot(nn_model2, rep = "best", main="")
grid::grid.text("Fig. 15 - Neural Network", x = .5, y = .2)
```

$~$

Below is a table displaying the predicted outcomes for the `PCOS` status, where the highest count of predictions corresponds to the absence of PCOS.

```{r, echo=FALSE}
# make predictions on the test data using a previously trained model
pred2 <- predict(nn_model2, valid2)

# create a vector of labels for the two possible `PCOS` status in the dataset.
labels2 <- c("0", "1")

# creates a data frame with the column index of the maximum value in each row of the "pred" variable
prediction_label2 <- data.frame(max.col(pred2)) %>% 
# use the mutate function to add a new column to the data frame called "pred"
mutate(pred=labels2[max.col.pred2.]) %>%
select(2) %>%
# convert the data frame to a vector.
unlist()

# print the table
table(valid2$`PCOS`, prediction_label2)
```

$~$

Lastly, we check the accuracy using the validation data by first converting actual categorical values into numerical ones and compare them with predicted values. The `neuralnet` accuracy is 30.37% which is oddly similar to the accuracy of the first model.
```{r, echo=FALSE}
# checking the accuracy
check2 <- as.numeric(valid2$`PCOS`) == max.col(pred2)
nn_accuracy2 <-  (sum(check2)/nrow(valid2))
nn_accuracy2
```


$~$

#### K-Nearest Neighbors (kknn or class):

To build the k-Nearest Neighbors model I prepared the data by removing rows with missing values, although there were none, the code wouldn't run without it. After setting a seed for reproducibility I trained a kNN model using the training data and predict the 'PCOS' variable for the validation dataset. The kNN algorithm allows tweaking the number of neighbors (k) to impact the outcome. Here, it was fixed at 5; as a result, the accuracy achieved was 71.11%.

```{r, echo=FALSE}
# Remove rows with missing values from train and valid datasets
train <- train[complete.cases(train), ]
valid <- valid[complete.cases(valid), ]

# set a seed for reproducibility purposes
set.seed(78)

# Set the value of k for kNN
k <- 5  # Change this value as needed

# Fit the kNN model using the training data
knn_model <- knn(train[, -which(names(train) == "PCOS")], 
                 valid[, -which(names(valid) == "PCOS")], 
                 train$`PCOS`, 
                 k = k)
```


```{r, echo=FALSE}
# Calculate accuracy
knn_accuracy <- mean(knn_model == valid$`PCOS`)
knn_accuracy
```

$~$

#### Second kNN:

For the second kNN, the same steps were takes as the first model with the exception of using the 6 variables of `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth` and target variable `PCOS`. The result, an accuracy of 88.15% was achieved, which is much higher than the first model.

```{r, echo=FALSE}
# Filter and select the desired columns for the new dataset
pcos_cleaned4 <- pcos_cleaned %>%
  select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)

# Split the data into training and validation sets (if needed)
set.seed(123)  # Set seed for reproducibility
inTrain4 <- createDataPartition(pcos_cleaned4$`PCOS`, p = 0.75, list = FALSE)
train4 <- pcos_cleaned4[inTrain4, ]
valid4 <- pcos_cleaned4[-inTrain4, ]

# Check for missing values and remove them if present
train4 <- train4[complete.cases(train4), ]
valid4 <- valid4[complete.cases(valid4), ]

# Set the value of k for kNN
k <- 5  # Change this value as needed

# Fit the kNN model using the training data
knn_model2 <- knn(train4[, -which(names(train4) == "PCOS")], 
                  valid4[, -which(names(valid4) == "PCOS")], 
                  train4$`PCOS`, 
                  k = k)
```

```{r, echo=FALSE}
# Calculate accuracy for the new kNN model
knn_accuracy2 <- mean(knn_model2 == valid4$`PCOS`)
knn_accuracy2
```


$~$

### Model Comparison:

Lastly, let's do a model comparison. Overall the top 5 models with the highest accuracies were: Decision Tree (second model), SVM (second and first model), Random Forest (second model), and Gradient Boost Machine (first model).


```{r, echo=FALSE}
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.0000, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
results <- data.frame(Model = model_names, Accuracy = accuracies)
results <- results[order(results$Accuracy, decreasing = TRUE), ]
head(results)
```


### Conclusion:


### References:





