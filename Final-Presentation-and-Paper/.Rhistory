theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45)
)
# plot all barcharts
ggp_all4 <- (p25 + p26 + p27) / (p28 + p29 + p30) / (p31 + p32 + p33) / (p34 + p35) +    # Create grid of plots with title
plot_annotation(title = "Fig. 9 - Bar charts of yes or no variables") &
theme(plot.title = element_text(hjust = 0.5))
ggp_all4
# Barchart for Vitamin D3 levels with PCOS
p36 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`Vit_D3_ngmL`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("Vitamin D3 levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# Barchart for FSH/LH levels with PCOS
p37 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`FSH_LH`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("FSH/LH levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# Barchart for Thyroid Hormone levels with PCOS
p38 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`TSH_mIUmL`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("Thyroid Hormone levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# Barchart of hemoglobin levels with PCOS
p39 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`Hb_gdl`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("Hemoglobin levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# Barchart of Prolactin levels with PCOS
p40 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`PRL_ngmL`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("Prolactin levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# Barchart of Progesterone levels with PCOS
p41 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`PRG_ngmL`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("Progesterone levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# Barchart of Glucose levels with PCOS
p42 <- pcos_cleaned %>%
ggplot(aes(x = as.factor(`RBS_mgdl`), fill = as.factor(`PCOS`))) +
geom_bar(position = "dodge") +
#geom_text(aes(label = ..count..), position = position_dodge(width = 1), stat = "count", vjust = 1.5, colour = "black") +
ggtitle("Glucose levels with PCOS") +
scale_fill_discrete(name = "PCOS", labels = c("No", "Yes")) +
theme_ipsum() +
theme(
plot.title = element_text(size=10), axis.text.x = element_text(angle = 45, size = 2)
)
# plot barcharts
ggp_all5 <- (p36 + p37 + p38 + p39) / (p40 + p41 + p42)  +    # Create grid of plots with title
plot_annotation(title = "Fig. 10 - Bar charts of Biometrics Measures including PCOS (Y/N) as factor") &
theme(plot.title = element_text(hjust = 0.5))
ggp_all5
# create some random numbers for reproduction
set.seed(29)
# Cross Validation Set-up
inTrain <- createDataPartition(pcos_cleaned$`PCOS`, p=.75, list = F)
train <- pcos_cleaned[inTrain,]
valid <- pcos_cleaned[-inTrain,]
# create the decision tree
rpart_model <- rpart(`PCOS` ~ ., method = "class", data = train)
# display the decision tree
prp(rpart_model, main = "Fig. 11 - Decision Tree with entire dataset", extra=1, faclen=0,  nn=T, box.palette="Blues")
# creating our prediction
rpart_result <- predict(rpart_model, newdata = valid[, !colnames(valid) %in% "PCOS"], type = 'class')
# confusion matrix
confusionMatrix(rpart_result, as.factor(valid$`PCOS`))
# contribution of variables
varImp(rpart_model) %>% kable()
# Extract accuracy from the confusion matrix
accuracy_rpart <- confusionMatrix(rpart_result, as.factor(valid$`PCOS`))$overall["Accuracy"]
kable(accuracy_rpart, align = "l")
# creating the second dataset from the original
pcos_cleaned2 <- pcos_cleaned %>%
select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)
# create some random number for reproduction
set.seed(28)
# Second Cross Validation Set-up
inTrain2 <- createDataPartition(pcos_cleaned2$`PCOS`, p=.75, list = F)
train2 <- pcos_cleaned2[inTrain2,]
valid2 <- pcos_cleaned2[-inTrain2,]
# create the second decision tree
rpart_model2 <- rpart(`PCOS` ~ ., method = "class", data = train2)
# display the decision tree
prp(rpart_model2, main = "Fig. 12 - Second Decision Tree with 6 variables", extra=1, faclen=0,  nn=T, box.palette="Blues")
# creating our prediction
rpart_result2 <- predict(rpart_model2, newdata = valid2[, !colnames(valid2) %in% "PCOS"], type = 'class')
# creating the second confusion matrix
confusionMatrix(rpart_result2, as.factor(valid2$`PCOS`))
# contribution of variables
varImp(rpart_model2) %>% kable()
# Extract accuracy from the confusion matrix
accuracy_rpart2 <- confusionMatrix(rpart_result2, as.factor(valid2$`PCOS`))$overall["Accuracy"]
kable(accuracy_rpart2, align = "l")
# create some random numbers for reproduction
set.seed(30)
# Cross Validation Set-up
rf_inTrain <- createDataPartition(pcos_cleaned$`PCOS`, p=.75, list = F)
rf_train <- pcos_cleaned[rf_inTrain,]
rf_valid <- pcos_cleaned[-rf_inTrain,]
# check the levels of PCOS using levels()
levels(rf_train$PCOS)
levels(rf_valid$PCOS)
# Convert PCOS to factor in rf_train
rf_train$PCOS <- factor(rf_train$PCOS)
# Convert PCOS to factor in rf_valid
rf_valid$PCOS <- factor(rf_valid$PCOS)
# rechecking levels again to ensure no NULL values
levels(rf_train$PCOS)
levels(rf_valid$PCOS)
# explicitly set the levels to match the levels in rf_train.
rf_valid$PCOS <- factor(rf_valid$PCOS, levels = levels(rf_train$PCOS))
levels(rf_valid$PCOS)
# # Check the length of svm_result and svm_valid$PCOS
# length_rf_result <- length(rf_result)
# length_rf_valid <- length(rf_valid$PCOS)
#
# # Print the lengths for comparison
# print(length_rf_result)
# print(length_rf_valid)
#create some random number for reproduction
set.seed(39)
# create random forest model using the training data
rf_model <- randomForest(PCOS~., rf_train)
rf_model
# prediction
rf_result <- predict(rf_model, newdata = valid[, !colnames(valid) %in% "PCOS"])
# Create a confusion matrix
confusionMatrix(data = rf_result, reference = rf_valid$PCOS)
# plot for rf_model fig 13
varImpPlot(rf_model)
# table for rf_model variable contribution
varImp(rf_model) %>% kable()
# Extract accuracy from the confusion matrix for the rf_model
accuracy_rf <- confusionMatrix(rf_result, valid$PCOS)$overall["Accuracy"]
accuracy_rf
# create some random numbers for reproduction
set.seed(78)
# Second RF Cross Validation Set-up
rf_inTrain2 <- createDataPartition(pcos_cleaned2$`PCOS`, p=.75, list = F)
rf_train2 <- pcos_cleaned2[rf_inTrain2,]
rf_valid2 <- pcos_cleaned2[-rf_inTrain2,]
# check the levels of PCOS using levels()
levels(rf_train2$PCOS)
levels(rf_valid2$PCOS)
# Convert PCOS to factor in rf_train
rf_train2$PCOS <- factor(rf_train2$PCOS)
# Convert PCOS to factor in rf_valid
rf_valid2$PCOS <- factor(rf_valid2$PCOS)
# rechecking levels again to ensure no NULL values
levels(rf_train2$PCOS)
levels(rf_valid2$PCOS)
# explicitly set the levels to match the levels in rf_train.
rf_valid2$PCOS <- factor(rf_valid2$PCOS, levels = levels(rf_train2$PCOS))
levels(rf_valid2$PCOS)
# # Check the length of rf_result and rf_valid$PCOS
# length_rf_result2 <- length(rf_result2)
# length_rf_valid2 <- length(rf_valid2$PCOS)
#
# # Print the lengths for comparison
# print(length_rf_result2)
# print(length_rf_valid2)
# create some random number for reproduction
set.seed(7)
# create the second random forest model using the training data from the third decision tree
rf_model2 <- randomForest(PCOS ~ Follicle_NoR + Follicle_NoL + Weight_gain + Skin_darkening + Hair_growth, data = rf_train2)
rf_model2
# creating the prediction for the third decision tree
rf_result2 <- predict(rf_model2, newdata = rf_valid2[, !colnames(rf_valid2) %in% "PCOS"])
# Convert PCOS column to factor in rf_train2 and rf_valid2
rf_train2$PCOS <- factor(rf_train2$PCOS)
rf_valid2$PCOS <- factor(rf_valid2$PCOS)
# # Check unique levels in rf_result2 and rf_valid2$PCOS
# unique_levels_result <- unique(rf_result2)
# unique_levels_valid <- unique(rf_valid2$PCOS)
#
# # Check if the levels match
# identical(unique_levels_result, unique_levels_valid)
#
# # If levels do not match, manually set levels in rf_result2 to match those in rf_valid2$PCOS
# levels(rf_result2) <- levels(rf_valid2$PCOS)
#
# Convert rf_result2 to factor and align levels with rf_valid2$PCOS
rf_result2_factor <- factor(rf_result2, levels = levels(rf_valid2$PCOS))
# Create a confusion matrix
confusionMatrix(data = rf_result2_factor, reference = rf_valid2$PCOS)
# plot for the second rf_model fig 14
varImpPlot(rf_model2)
# table for rf_model2 variable contribution
varImp(rf_model2) %>% kable()
# Extract accuracy from the confusion matrix for the rf_model2
accuracy_rf2 <- confusionMatrix(data = rf_result2_factor, reference = rf_valid2$PCOS)$overall["Accuracy"]
accuracy_rf2
# Set seed for reproducibility
set.seed(67)
# Train the GBM model
gbm_model <- gbm(`PCOS` ~ ., data = train, distribution = "bernoulli", n.trees = 100, interaction.depth = 4, shrinkage = 0.01, bag.fraction = 0.5)
# Print the summary of the trained model
summary(gbm_model)
# Predict on the validation dataset (assuming 'valid' contains your validation dataset) fig 15
gbm_pred <- predict(gbm_model, newdata = valid, type = "response")
# Calculate predicted classes (0 or 1) based on the predicted probabilities
predicted_classes <- ifelse(gbm_pred > 0.5, 1, 0)
# Create confusion matrix
confusionMatrix(data = factor(predicted_classes), reference = factor(valid$`PCOS`))
# Calculate accuracy
gbm_accuracy <- sum(predicted_classes == valid$`PCOS`) / length(valid$`PCOS`)
cat("Accuracy:", gbm_accuracy)
# creating the second dataset from the original
pcos_cleaned3 <- pcos_cleaned %>%
select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)
# Set seed for reproducibility
set.seed(68)
# Cross Validation Set-up
inTrain3 <- createDataPartition(pcos_cleaned3$`PCOS`, p=.75, list = F)
train3 <- pcos_cleaned3[inTrain3,]
valid3 <- pcos_cleaned3[-inTrain3,]
# Train the GBM model
gbm_model2 <- gbm(`PCOS` ~ ., data = train3, distribution = "bernoulli", n.trees = 100, interaction.depth = 4, shrinkage = 0.01, bag.fraction = 0.5)
# Print the summary of the trained model
summary(gbm_model2)
# Predict on the validation dataset (assuming 'valid' contains your validation dataset) fig 16
gbm_pred2 <- predict(gbm_model2, newdata = valid3, type = "response")
# Calculate predicted classes (0 or 1) based on the predicted probabilities
predicted_classes2 <- ifelse(gbm_pred2 > 0.5, 1, 0)
# Create confusion matrix
confusionMatrix(data = factor(predicted_classes2), reference = factor(valid3$`PCOS`))
# Calculate accuracy
gbm_accuracy2 <- sum(predicted_classes2 == valid3$`PCOS`) / length(valid3$`PCOS`)
cat("Accuracy:", gbm_accuracy2)
# check the levels of PCOS using levels()
levels(train$PCOS)
levels(valid$PCOS)
# Convert PCOS to factor in svm_train
train$PCOS <- factor(train$PCOS)
# Convert PCOS to factor in svm_valid
valid$PCOS <- factor(valid$PCOS)
# rechecking levels again to ensure no NULL values
levels(train$PCOS)
levels(valid$PCOS)
# checking the structure of both valid and train datasets
str(valid)
str(train)
# explicitly set the levels to match the levels in svm_train.
valid$PCOS <- factor(valid$PCOS, levels = levels(train$PCOS))
levels(valid$PCOS)
# # Check the length of svm_result and svm_valid$PCOS
# length_svm_result <- length(svm_result)
# length_svm_valid <- length(svm_valid$PCOS)
#
# # Print the lengths for comparison
# print(length_svm_result)
# print(length_svm_valid)
#create some random numbers for reproduction
set.seed(31)
# SVM
svm_model <- svm(PCOS ~ ., train)
# create prediction
svm_result <- predict(svm_model, newdata = valid)
# confusion matrix for svm
confusionMatrix(svm_result, valid$PCOS)
# summary of svm_result
summary(svm_result)
#Extract accuracy from the confusion matrix
accuracy_svm <- confusionMatrix(svm_result, as.factor(valid$`PCOS`))$overall["Accuracy"]
accuracy_svm
# create some random numbers for reproduction
set.seed(8)
# Cross Validation Set-up
svm_inTrain2 <- createDataPartition(pcos_cleaned2$PCOS, p=.75, list = FALSE)
svm_train2 <- pcos_cleaned2[svm_inTrain2,]
svm_valid2 <- pcos_cleaned2[-svm_inTrain2,]
# check the levels of PCOS using levels()
levels(svm_train2$PCOS)
levels(svm_valid2$PCOS)
# Convert PCOS to factor in svm_train2
svm_train2$PCOS <- factor(svm_train2$PCOS)
# Convert PCOS to factor in svm_valid2
svm_valid2$PCOS <- factor(svm_valid2$PCOS)
# rechecking levels again to ensure no NULL values
levels(svm_train2$PCOS)
levels(svm_valid2$PCOS)
# explicitly set the levels to match the levels in svm_train2
valid$PCOS <- factor(svm_valid2$PCOS, levels = levels(svm_train2$PCOS))
levels(svm_valid2$PCOS)
# # Check the length of svm_result and svm_valid$PCOS
# length_svm_result <- length(svm_result)
# length_svm_valid2 <- length(svm_valid2$PCOS)
#
# # Print the lengths for comparison
# print(length_svm_result)
# print(length_svm_valid2)
# Second SVM
svm_model2 <- svm(PCOS ~ Follicle_NoR + Follicle_NoL + Weight_gain + Skin_darkening + Hair_growth, svm_train2)
# create prediction
svm_result2 <- predict(svm_model2, newdata = svm_valid2)
# confusion matrix for svm_valid2
confusionMatrix(svm_result2, svm_valid2$PCOS)
# summary
summary(svm_result2)
#Extract accuracy from the confusion matrix
accuracy_svm2 <- confusionMatrix(svm_result2, svm_valid2$`PCOS`)$overall["Accuracy"]
accuracy_svm2
# create some random numbers for reproduction
set.seed(67)
# Cross Validation Set-up
nn_inTrain <- createDataPartition(pcos_cleaned$PCOS, p=.75, list = F)
nn_train <- pcos_cleaned[nn_inTrain,]
nn_valid <- pcos_cleaned[-nn_inTrain,]
# set a seed for reproducibility purposes
set.seed(19)
# create the model
nn_model <- neuralnet(`PCOS`~.,
data = nn_train,
hidden = c(12, 8),  # Specify the number of hidden layers and neurons
linear.output = FALSE,
stepmax = 20000  # Increase the maximum number of iterations
)
# create the plot based on the model above
#plot(nn_model, rep = "best", main="")
#grid::grid.text("Fig. 15 - Neural Network", x = 0.5, y = 0.1)
# make predictions on the test data using a previously trained model
pred <- predict(nn_model, valid)
# create a vector of labels for the two possible `PCOS(Y/N)` status in the dataset.
labels <- c("0", "1")
# creates a data frame with the column index of the maximum value in each row of the "pred" variable
prediction_label <- data.frame(max.col(pred)) %>%
# use the mutate function to add a new column to the data frame called "pred"
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
# convert the data frame to a vector.
unlist()
# print the table
table(valid$`PCOS`, prediction_label)
#checking the accuracy
check <- as.numeric(valid$`PCOS`) == max.col(pred)
nn_accuracy <-  (sum(check)/nrow(valid))
nn_accuracy
# set a seed for reproducibility purposes
set.seed(13)
# create the second model
nn_model2 <-  neuralnet(`PCOS`~Follicle_NoL + Follicle_NoR + Hair_growth + Skin_darkening + Weight_gain,
data=train2,
hidden=c(2,1),
linear.output = FALSE,
stepmax = 10000  # Increase the maximum number of iterations
)
# create the plot based on the model above
plot(nn_model2, rep = "best", main="")
grid::grid.text("Fig. 18 -  Second Neural Network", x = .5, y = .2)
# make predictions on the test data using a previously trained model
pred2 <- predict(nn_model2, valid2)
# create a vector of labels for the two possible `PCOS` status in the dataset.
labels2 <- c("0", "1")
# creates a data frame with the column index of the maximum value in each row of the "pred" variable
prediction_label2 <- data.frame(max.col(pred2)) %>%
# use the mutate function to add a new column to the data frame called "pred"
mutate(pred=labels2[max.col.pred2.]) %>%
select(2) %>%
# convert the data frame to a vector.
unlist()
# print the table
table(valid2$`PCOS`, prediction_label2)
# checking the accuracy
check2 <- as.numeric(valid2$`PCOS`) == max.col(pred2)
nn_accuracy2 <-  (sum(check2)/nrow(valid2))
nn_accuracy2
# Remove rows with missing values from train and valid datasets
train <- train[complete.cases(train), ]
valid <- valid[complete.cases(valid), ]
# set a seed for reproducibility purposes
set.seed(78)
# Set the value of k for kNN
k <- 5  # Change this value as needed
# Fit the kNN model using the training data
knn_model <- knn(train[, -which(names(train) == "PCOS")],
valid[, -which(names(valid) == "PCOS")],
train$`PCOS`,
k = k)
# Calculate accuracy
knn_accuracy <- mean(knn_model == valid$`PCOS`)
knn_accuracy
# Filter and select the desired columns for the new dataset
pcos_cleaned4 <- pcos_cleaned %>%
select(`PCOS`, `Follicle_NoR`, `Follicle_NoL`, `Weight_gain`, `Skin_darkening`, `Hair_growth`)
# Split the data into training and validation sets (if needed)
set.seed(123)  # Set seed for reproducibility
inTrain4 <- createDataPartition(pcos_cleaned4$`PCOS`, p = 0.75, list = FALSE)
train4 <- pcos_cleaned4[inTrain4, ]
valid4 <- pcos_cleaned4[-inTrain4, ]
# Check for missing values and remove them if present
train4 <- train4[complete.cases(train4), ]
valid4 <- valid4[complete.cases(valid4), ]
# Set the value of k for kNN
k <- 5  # Change this value as needed
# Fit the kNN model using the training data
knn_model2 <- knn(train4[, -which(names(train4) == "PCOS")],
valid4[, -which(names(valid4) == "PCOS")],
train4$`PCOS`,
k = k)
# Calculate accuracy for the new kNN model
knn_accuracy2 <- mean(knn_model2 == valid4$`PCOS`)
knn_accuracy2
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kable(caption = "<font color=#000000><b>Table 5.</b>`Model Comparison </font>", format = "html", col.names = colnames(results)) %>%
kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
kableExtra::scroll_box(width = "100%", height = "400px")
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kables(caption = "<font color=#000000><b>Table 5.</b>`Model Comparison </font>", format = "html", col.names = colnames(results)) %>%
kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
kableExtra::scroll_box(width = "100%", height = "400px")
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kable(results)
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kable(resultscaption = "<font color=#000000><b>Table 5.</b>Model Comparison </font>", format = "html")
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kable(results, caption = "<font color=#000000><b>Table 5.</b>Model Comparison </font>", format = "html")
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kable(results, caption = "<font color=#000000><b>Table 5.</b>Model Comparison </font>", format = "html") %>%
kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
kableExtra::scroll_box(width = "100%", height = "400px")
# Compare models
model_names <- c("Decision Tree 1","Decision Tree 2", "Random Forest 1", "Random Forest 2", "Gradient Boost Machines 1", "Gradient Boost Machines 2" ,"SVM 1", "SVM 2", "Neural Network 1", "Neural Network 2", "k-Nearest 1", "k-Nearest 2")
accuracies <- c(0.8592593, 0.9185185, 0.6148148, 0.9037037, 0.8814815, 0.6222222, 0.9111111, 0.9185185, 0.3037037, 0.3037037, 0.7111111, 0.8814815)
# place accuracies in data frame
results <- data.frame(Model = model_names, Accuracy = accuracies)
# order in descending order
results <- results[order(results$Accuracy, decreasing = TRUE), ]
# Display the results
kable(results, caption = "<font color=#000000><b>Table 5. </b>Model Comparison </font>", format = "html") %>%
kable_styling(bootstrap_options = c("hover", "condensed"), font_size = 13) %>%
kableExtra::scroll_box(width = "100%", height = "400px")
